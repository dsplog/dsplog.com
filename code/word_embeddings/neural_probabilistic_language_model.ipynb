{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f872da38",
   "metadata": {},
   "source": [
    "### Neural Probabilistic Language Model, Bengio et al. (2003) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Neural Probabilistic Language Model \n",
    "# Bengio et al. (2003) \n",
    "# =========================\n",
    "\n",
    "# 1. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 2. Config\n",
    "n = 4        # context size = n-1 previous words + 1 target\n",
    "m = 10       # embedding dimension [m x 1]\n",
    "h = 16       # hidden layer dimension [h x 1]\n",
    "epochs = 10\n",
    "lr = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c8e88",
   "metadata": {},
   "source": [
    "### Toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d03db988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 42\n",
      "Number of training samples: 22\n",
      "Example context: [39, 9, 30] -> target: 12\n",
      "Example context words: ['the', 'cat', 'sat']\n",
      "Example target word: down\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 3. Toy Corpus (~20 sentences)\n",
    "# -----------------------------\n",
    "corpus = [\n",
    "    \"the cat sat down\",\n",
    "    \"the cat ate food\",\n",
    "    \"the dog sat down\",\n",
    "    \"the dog ate food\",\n",
    "    \"a cat chased a mouse\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"a dog barked loudly\",\n",
    "    \"the cat meowed softly\",\n",
    "    \"the bird sang sweetly\",\n",
    "    \"a bird flew away\",\n",
    "    \"the fish swam fast\",\n",
    "    \"a fish jumped high\",\n",
    "    \"the boy played ball\",\n",
    "    \"the girl sang song\",\n",
    "    \"a boy read book\",\n",
    "    \"a girl wrote letter\",\n",
    "    \"the sun shines bright\",\n",
    "    \"the moon glows softly\",\n",
    "    \"the stars twinkle bright\",\n",
    "    \"a cat slept quietly\"\n",
    "]\n",
    "\n",
    "\n",
    "# 4. Preprocessing\n",
    "tokens = set(\" \".join(corpus).split())\n",
    "word2idx = {word: i for i, word in enumerate(sorted(tokens))}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "V = len(word2idx)   # vocabulary size |V|\n",
    "\n",
    "# make context-target pairs for n-gram model\n",
    "def make_ngrams(corpus, n):\n",
    "    X, y = [], []\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words) - n):\n",
    "            context = words[i:i+n]\n",
    "            target = words[i+n]\n",
    "            X.append([word2idx[w] for w in context])\n",
    "            y.append(word2idx[target])\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "X, y = make_ngrams(corpus, n-1)\n",
    "\n",
    "\n",
    "print(\"Vocabulary size:\", V)\n",
    "print(\"Number of training samples:\", len(X))\n",
    "print('Example context:', X[0].tolist(), '-> target:', y[0].item())\n",
    "print('Example context words:', [idx2word[i] for i in X[0].tolist()])\n",
    "print('Example target word:', idx2word[y[0].item()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd51b29",
   "metadata": {},
   "source": [
    "### Model architecture and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0f37161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "==================\n",
      "Vocabulary size (V): 42\n",
      "Context size (n-1): 3\n",
      "Embedding dim (m): 10\n",
      "Hidden dim (h): 16\n",
      "\n",
      "Model Parameters:\n",
      "==================\n",
      "Embedding matrix (C): 420\n",
      "Context matrix (W): 1,260\n",
      "Hidden matrix (H): 480\n",
      "Output matrix (U): 672\n",
      "Biases (b, d): 58\n",
      "Total parameters: 2,890\n"
     ]
    }
   ],
   "source": [
    "# 5. Dataset/Dataloader\n",
    "class NGramDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(NGramDataset(X, y), batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "# 6. Model definition \n",
    "class NPLM(nn.Module):\n",
    "    def __init__(self, V, m, n, h):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding matrix C: maps word index to [m x 1] vector\n",
    "        # C(w) ∈ R^{m}\n",
    "        self.C = nn.Embedding(V, m)\n",
    "        \n",
    "        # W ∈ R^{[V x (n-1)·m]}, no bias\n",
    "        self.W = nn.Linear((n-1)*m, V, bias=False)\n",
    "        \n",
    "        # H ∈ R^{[h x (n-1)·m]}, no bias\n",
    "        self.H = nn.Linear((n-1)*m, h, bias=False)\n",
    "        \n",
    "        # U ∈ R^{[V x h]}, no bias\n",
    "        self.U = nn.Linear(h, V, bias=False)\n",
    "        \n",
    "        # Bias terms as separate parameters\n",
    "        # b ∈ R^{[V x 1]}, d ∈ R^{[h x 1]}\n",
    "        self.b = nn.Parameter(torch.zeros(V))\n",
    "        self.d = nn.Parameter(torch.zeros(h))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Step 1: Lookup embeddings for each context word\n",
    "        # C(w_{t-1}), ..., C(w_{t-n+1})\n",
    "        embeddings = self.C(x)  # [batch, n-1, m]\n",
    "        \n",
    "        # Step 2: Flatten into single vector\n",
    "        # x_flat ∈ R^{[(n-1)·m x 1]}\n",
    "        x_flat = embeddings.view(embeddings.size(0), -1)\n",
    "        \n",
    "        # Step 3: Hidden computation\n",
    "        # h_tanh = tanh(d + H · x_flat)\n",
    "        h_tanh = torch.tanh(self.d + self.H(x_flat))  # [batch, h]\n",
    "        \n",
    "        # Step 4: Output logits\n",
    "        # y = b + W · x_flat + U · h_tanh\n",
    "        y = self.b + self.W(x_flat) + self.U(h_tanh)  # [batch, V]\n",
    "        \n",
    "        # Step 5: Softmax for probabilities\n",
    "        # P̂(w_t | context) = exp(y_wt) / Σ_i exp(y_i)\n",
    "        # Ensures:\n",
    "        # 1. f > 0 for all sequences\n",
    "        # 2. Σ_i f(...) = 1\n",
    "        return torch.log_softmax(y, dim=1)\n",
    "\n",
    "# After model instantiation, add parameter counting\n",
    "model = NPLM(V, m, n, h)\n",
    "\n",
    "# Print model architecture and parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(\"==================\")\n",
    "print(f\"Vocabulary size (V): {V}\")\n",
    "print(f\"Context size (n-1): {n-1}\")\n",
    "print(f\"Embedding dim (m): {m}\")\n",
    "print(f\"Hidden dim (h): {h}\")\n",
    "print(\"\\nModel Parameters:\")\n",
    "print(\"==================\")\n",
    "print(f\"Embedding matrix (C): {V*m:,d}\")\n",
    "print(f\"Context matrix (W): {V*(n-1)*m:,d}\")\n",
    "print(f\"Hidden matrix (H): {h*(n-1)*m:,d}\")\n",
    "print(f\"Output matrix (U): {V*h:,d}\")\n",
    "print(f\"Biases (b, d): {V + h:,d}\")\n",
    "print(f\"Total parameters: {count_parameters(model):,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c61a56",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fafee978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 23.0056\n",
      "Epoch 2, Loss: 14.9684\n",
      "Epoch 3, Loss: 9.8470\n",
      "Epoch 4, Loss: 6.3263\n",
      "Epoch 5, Loss: 3.8484\n",
      "Epoch 6, Loss: 2.4036\n",
      "Epoch 7, Loss: 1.6115\n",
      "Epoch 8, Loss: 0.9805\n",
      "Epoch 9, Loss: 0.6869\n",
      "Epoch 10, Loss: 0.4971\n"
     ]
    }
   ],
   "source": [
    "# 7. Training loop\n",
    "criterion = nn.NLLLoss()  # since we used log_softmax\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(context)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26094c71",
   "metadata": {},
   "source": [
    "### Probability of next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "badeeb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: ['the', 'cat', 'sat']\n",
      "  1. 'down' (0.92)\n",
      "  2. 'softly' (0.04)\n",
      "  3. 'food' (0.01)\n",
      "  4. 'ball' (0.01)\n",
      "  5. 'sweetly' (0.01)\n",
      "\n",
      "Context: ['a', 'cat', 'chased']\n",
      "  1. 'a' (0.94)\n",
      "  2. 'the' (0.02)\n",
      "  3. 'quietly' (0.01)\n",
      "  4. 'high' (0.01)\n",
      "  5. 'softly' (0.01)\n",
      "\n",
      "Context: ['a', 'dog', 'barked']\n",
      "  1. 'loudly' (0.94)\n",
      "  2. 'a' (0.01)\n",
      "  3. 'letter' (0.01)\n",
      "  4. 'away' (0.01)\n",
      "  5. 'mouse' (0.01)\n",
      "\n",
      "Context: ['the', 'stars', 'glows']\n",
      "  1. 'softly' (0.38)\n",
      "  2. 'fast' (0.17)\n",
      "  3. 'bright' (0.14)\n",
      "  4. 'ball' (0.07)\n",
      "  5. 'food' (0.04)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6. Prediction Function\n",
    "# -----------------------------\n",
    "def predict_next(context_words, k=5):\n",
    "    context_idxs = torch.tensor([[word2idx[w] for w in context_words]])\n",
    "    with torch.no_grad():\n",
    "        output = model(context_idxs)\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        top_probs, top_indices = torch.topk(probs, k, dim=1)\n",
    "\n",
    "        \n",
    "        \n",
    "    # Get words and probabilities for top k predictions\n",
    "    predictions = [(idx2word[idx.item()], prob.item()) \n",
    "                  for idx, prob in zip(top_indices[0], top_probs[0])]\n",
    "    return predictions\n",
    "\n",
    "# Test a few predictions\n",
    "test_contexts = [\n",
    "    [\"the\", \"cat\", \"sat\"],\n",
    "    [\"a\", \"cat\", \"chased\"],\n",
    "    [\"a\",\"dog\",\"barked\"],\n",
    "    [\"the\",\"stars\",\"glows\"]\n",
    "]\n",
    "\n",
    "for context in test_contexts:\n",
    "    predictions = predict_next(context)\n",
    "    print(f\"\\nContext: {context}\")\n",
    "    for i, (word, prob) in enumerate(predictions, 1):\n",
    "        print(f\"  {i}. '{word}' ({prob:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
