{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f1036452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CBOW with Negative Sampling\n",
    "# =========================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import random\n",
    "\n",
    "# 1. Hyperparameters\n",
    "embedding_dim = 10\n",
    "context_size = 2   # number of words on each side\n",
    "num_negatives = 5  # number of negative samples\n",
    "epochs = 50\n",
    "lr = 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "948c4690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Toy Corpus (~20 sentences)\n",
    "# -----------------------------\n",
    "corpus = [\n",
    "    \"the cat sat down\",\n",
    "    \"the cat ate food\",\n",
    "    \"the dog sat down\",\n",
    "    \"the dog ate food\",\n",
    "    \"a cat chased a mouse\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"a dog barked loudly\",\n",
    "    \"the cat meowed softly\",\n",
    "    \"the bird sang sweetly\",\n",
    "    \"a bird flew away\",\n",
    "    \"the fish swam fast\",\n",
    "    \"a fish jumped high\",\n",
    "    \"the boy played ball\",\n",
    "    \"the girl sang song\",\n",
    "    \"a boy read book\",\n",
    "    \"a girl wrote letter\",\n",
    "    \"the sun shines bright\",\n",
    "    \"the moon glows softly\",\n",
    "    \"the stars twinkle bright\",\n",
    "    \"a cat slept quietly\"\n",
    "]\n",
    "\n",
    "PAD = \"<PAD>\"\n",
    "\n",
    "# 2. Vocabulary\n",
    "tokens = sorted(list(set(\" \".join(corpus).split()))) + [PAD]\n",
    "word2idx = {w: i for i, w in enumerate(tokens)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "PAD_idx = word2idx[PAD]\n",
    "\n",
    "V = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "52300133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word       |    Noise Probability\n",
      "-----------------------------------\n",
      "a          |             0.073365\n",
      "ate        |             0.025938\n",
      "away       |             0.015423\n",
      "ball       |             0.015423\n",
      "barked     |             0.015423\n",
      "bird       |             0.025938\n",
      "book       |             0.015423\n",
      "boy        |             0.025938\n",
      "bright     |             0.025938\n",
      "cat        |             0.059127\n",
      "chased     |             0.025938\n",
      "dog        |             0.043623\n",
      "down       |             0.025938\n",
      "fast       |             0.015423\n",
      "fish       |             0.025938\n",
      "flew       |             0.015423\n",
      "food       |             0.025938\n",
      "girl       |             0.025938\n",
      "glows      |             0.015423\n",
      "high       |             0.015423\n",
      "jumped     |             0.015423\n",
      "letter     |             0.015423\n",
      "loudly     |             0.015423\n",
      "meowed     |             0.015423\n",
      "moon       |             0.015423\n",
      "mouse      |             0.015423\n",
      "played     |             0.015423\n",
      "quietly    |             0.015423\n",
      "read       |             0.015423\n",
      "sang       |             0.025938\n",
      "sat        |             0.025938\n",
      "shines     |             0.015423\n",
      "slept      |             0.015423\n",
      "softly     |             0.025938\n",
      "song       |             0.015423\n",
      "stars      |             0.015423\n",
      "sun        |             0.015423\n",
      "swam       |             0.015423\n",
      "sweetly    |             0.015423\n",
      "the        |             0.111626\n",
      "twinkle    |             0.015423\n",
      "wrote      |             0.015423\n",
      "<PAD>      |             0.000000\n"
     ]
    }
   ],
   "source": [
    "# 3. Noise distribution for negative sampling\n",
    "import collections\n",
    "counts = collections.Counter(\" \".join(corpus).split())\n",
    "total = sum(counts.values())\n",
    "freqs = torch.tensor([counts[w]/total for w in tokens], dtype=torch.float)\n",
    "# Use unigram^3/4 for negative sampling (Mikolov et al.)\n",
    "noise_dist = freqs ** 0.75\n",
    "noise_dist = noise_dist / noise_dist.sum()\n",
    "\n",
    "# Print the noise distribution with words\n",
    "print(f\"{'Word':<10} | {'Noise Probability':>20}\")\n",
    "print(\"-\" * 35)\n",
    "for w, p in zip(tokens, noise_dist):\n",
    "    print(f\"{w:<10} | {p.item():>20.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a8253203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CBOW pairs: 82\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 4. Generate CBOW pairs\n",
    "# --------------------------\n",
    "def generate_cbow_pairs(corpus, context_size=2):\n",
    "    pairs = []\n",
    "    window_len = 2 * context_size\n",
    "\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        N = len(words)\n",
    "\n",
    "        for i, target in enumerate(words):\n",
    "            context_ids = []\n",
    "\n",
    "            # fill left + right context positions\n",
    "            for offset in range(-context_size, context_size + 1):\n",
    "                if offset == 0:\n",
    "                    continue  # skip target\n",
    "                j = i + offset\n",
    "                if 0 <= j < N:\n",
    "                    context_ids.append(word2idx[words[j]])\n",
    "                else:\n",
    "                    context_ids.append(PAD_idx)  # pad\n",
    "\n",
    "            # now context_ids is always length 2*context_size\n",
    "            pairs.append((context_ids, word2idx[target]))\n",
    "\n",
    "    return pairs\n",
    "pairs = generate_cbow_pairs(corpus, context_size)\n",
    "\n",
    "print(f\"Total CBOW pairs: {len(pairs)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3b6a9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# 5. Dataset\n",
    "# --------------------------\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        contexts, target = self.pairs[idx]\n",
    "        return torch.tensor(contexts), torch.tensor(target)\n",
    "\n",
    "dataset = CBOWDataset(pairs)\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "11bf7dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Example DataLoader Iteration ===\n",
      "\n",
      "Batch 1:\n",
      "  Context: ['the', 'cat', 'down', '<PAD>'] → Target: 'sat'\n",
      "  Context: ['<PAD>', 'a', 'flew', 'away'] → Target: 'bird'\n",
      "  Context: ['a', 'girl', 'letter', '<PAD>'] → Target: 'wrote'\n",
      "\n",
      "Batch 2:\n",
      "  Context: ['<PAD>', 'a', 'barked', 'loudly'] → Target: 'dog'\n",
      "  Context: ['<PAD>', 'a', 'chased', 'a'] → Target: 'cat'\n",
      "  Context: ['<PAD>', '<PAD>', 'cat', 'chased'] → Target: 'a'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Example DataLoader Iteration ===\")\n",
    "for batch_i, (contexts, targets) in enumerate(dataloader):\n",
    "    print(f\"\\nBatch {batch_i+1}:\")\n",
    "    for c, t in zip(contexts, targets):\n",
    "        ctx_words = [idx2word[i.item()] for i in c]\n",
    "        tgt_word = idx2word[t.item()]\n",
    "        print(f\"  Context: {ctx_words} → Target: '{tgt_word}'\")\n",
    "    if batch_i == 1:  # just print 2 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4a07e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 6. Model\n",
    "# --------------------------\n",
    "class CBOWNegativeSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, PAD_idx):\n",
    "        super().__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.PAD_idx = PAD_idx\n",
    "\n",
    "\n",
    "    def forward(self, contexts, target, neg_samples):\n",
    "\n",
    "        # ------------------------\n",
    "        # MASKED CONTEXT AVERAGE\n",
    "        # ------------------------\n",
    "        mask = (contexts != self.PAD_idx).float()         # [B, 2C]\n",
    "        embeds = self.in_embed(contexts)                  # [B, 2C, D]\n",
    "        masked_embeds = embeds * mask.unsqueeze(2)        # [B, 2C, D]\n",
    "\n",
    "        sum_embeds = masked_embeds.sum(dim=1)             # [B, D]\n",
    "        count = mask.sum(dim=1).unsqueeze(1)              # [B, 1]\n",
    "        count = torch.clamp(count, min=1)\n",
    "\n",
    "        v_c = sum_embeds / count                           # [B, D]\n",
    "\n",
    "        # ------------------------\n",
    "        # POSITIVE SCORE\n",
    "        # ------------------------\n",
    "        u_o = self.out_embed(target)                      # [B, D]\n",
    "        pos_score = (v_c * u_o).sum(dim=1)\n",
    "        pos_loss = F.logsigmoid(pos_score)\n",
    "\n",
    "        # ------------------------\n",
    "        # NEGATIVE SCORE\n",
    "        # ------------------------\n",
    "        u_k = self.out_embed(neg_samples)                 # [B, K, D]\n",
    "        neg_score = torch.bmm(u_k, v_c.unsqueeze(2)).squeeze(dim=2)\n",
    "        neg_loss = F.logsigmoid(-neg_score).sum(1)\n",
    "\n",
    "        return -(pos_loss + neg_loss).mean()        \n",
    "\n",
    "model = CBOWNegativeSampling(V, embedding_dim, PAD_idx)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2a6bb591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Parameters ===\n",
      "in_embed.weight      |        430 params\n",
      "out_embed.weight     |        430 params\n",
      "-------------------------------------------------------\n",
      "Total Trainable Params: 860\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    print(\"\\n=== Model Parameters ===\")\n",
    "    total_params = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            n_params = param.numel()\n",
    "            mem = n_params * param.element_size()  # bytes\n",
    "            total_params += n_params\n",
    "            print(f\"{name:20s} | {n_params:10,d} params\")\n",
    "\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"Total Trainable Params: {total_params:,}\")\n",
    "\n",
    "count_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "590e6810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 171.6441\n",
      "Epoch 2/50, Loss: 171.6962\n",
      "Epoch 3/50, Loss: 144.1562\n",
      "Epoch 4/50, Loss: 148.6950\n",
      "Epoch 5/50, Loss: 124.0212\n",
      "Epoch 6/50, Loss: 124.3845\n",
      "Epoch 7/50, Loss: 126.8215\n",
      "Epoch 8/50, Loss: 117.6695\n",
      "Epoch 9/50, Loss: 108.4824\n",
      "Epoch 10/50, Loss: 102.3632\n",
      "Epoch 11/50, Loss: 98.7818\n",
      "Epoch 12/50, Loss: 92.5184\n",
      "Epoch 13/50, Loss: 80.7378\n",
      "Epoch 14/50, Loss: 80.4390\n",
      "Epoch 15/50, Loss: 75.0329\n",
      "Epoch 16/50, Loss: 69.5878\n",
      "Epoch 17/50, Loss: 65.3495\n",
      "Epoch 18/50, Loss: 67.0577\n",
      "Epoch 19/50, Loss: 62.1975\n",
      "Epoch 20/50, Loss: 55.8747\n",
      "Epoch 21/50, Loss: 58.2444\n",
      "Epoch 22/50, Loss: 55.1815\n",
      "Epoch 23/50, Loss: 51.1636\n",
      "Epoch 24/50, Loss: 55.6556\n",
      "Epoch 25/50, Loss: 49.2184\n",
      "Epoch 26/50, Loss: 49.9983\n",
      "Epoch 27/50, Loss: 48.2944\n",
      "Epoch 28/50, Loss: 46.1428\n",
      "Epoch 29/50, Loss: 43.7625\n",
      "Epoch 30/50, Loss: 43.7921\n",
      "Epoch 31/50, Loss: 42.0808\n",
      "Epoch 32/50, Loss: 43.3042\n",
      "Epoch 33/50, Loss: 39.9031\n",
      "Epoch 34/50, Loss: 39.3197\n",
      "Epoch 35/50, Loss: 42.5963\n",
      "Epoch 36/50, Loss: 37.7143\n",
      "Epoch 37/50, Loss: 36.5981\n",
      "Epoch 38/50, Loss: 36.6528\n",
      "Epoch 39/50, Loss: 39.3041\n",
      "Epoch 40/50, Loss: 34.8198\n",
      "Epoch 41/50, Loss: 32.5827\n",
      "Epoch 42/50, Loss: 38.8210\n",
      "Epoch 43/50, Loss: 34.1500\n",
      "Epoch 44/50, Loss: 33.6495\n",
      "Epoch 45/50, Loss: 34.2227\n",
      "Epoch 46/50, Loss: 36.0112\n",
      "Epoch 47/50, Loss: 34.3870\n",
      "Epoch 48/50, Loss: 34.7694\n",
      "Epoch 49/50, Loss: 34.2668\n",
      "Epoch 50/50, Loss: 31.7492\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for contexts, target in dataloader:\n",
    "        neg_samples = torch.multinomial(noise_dist, len(target) * num_negatives, replacement=True)\n",
    "        neg_samples = neg_samples.view(len(target), num_negatives)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(contexts, target, neg_samples)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f9336e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 cosine-similar context words for 'cat':\n",
      "Word         | Cosine Sim\n",
      "----------------------------\n",
      "down         |     0.1469\n",
      "the          |     0.1242\n",
      "slept        |     0.0764\n",
      "a            |     0.0730\n",
      "chased       |     0.0311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def predict_top_context(center_word, top_k=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # get index of the center word\n",
    "        center_idx = torch.tensor([word2idx[center_word]])\n",
    "        # get embedding of the center word\n",
    "        v_c = model.in_embed(center_idx)  # [1, D]\n",
    "        v_c_norm = v_c / v_c.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # get all output embeddings and normalize\n",
    "        u_all = model.out_embed.weight  # [V, D]\n",
    "        u_all_norm = u_all / u_all.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # cosine similarity\n",
    "        scores = torch.matmul(u_all_norm, v_c_norm.t()).squeeze()  # [V]\n",
    "\n",
    "        # exclude center word itself\n",
    "        scores[word2idx[center_word]] = -float('inf')\n",
    "        topk_scores, topk_idx = torch.topk(scores, top_k)\n",
    "\n",
    "        top_words = [idx2word[i.item()] for i in topk_idx]\n",
    "        top_scores = [s.item() for s in topk_scores]\n",
    "\n",
    "        print(f\"\\nTop {top_k} cosine-similar context words for '{center_word}':\")\n",
    "        print(f\"{'Word':<12} | {'Cosine Sim':>10}\")\n",
    "        print(\"-\" * 28)\n",
    "        for w, s in zip(top_words, top_scores):\n",
    "            print(f\"{w:<12} | {s:>10.4f}\")\n",
    "        print()\n",
    "\n",
    "        return list(zip(top_words, top_scores))\n",
    "\n",
    "# Example usage\n",
    "center = 'cat'\n",
    "_ = predict_top_context(center, top_k=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
