{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4c49122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# GloVe Embedding Model\n",
    "# =========================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "\n",
    "# 1. Hyperparameters\n",
    "embedding_dim = 10\n",
    "x_max = 100  # cutoff in weighting function\n",
    "alpha = 0.75  # exponent in weighting function\n",
    "epochs = 25\n",
    "lr = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaab42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Toy Corpus (~20 sentences)\n",
    "corpus = [\n",
    "    \"the cat sat down\",\n",
    "    \"the cat ate food\",\n",
    "    \"the dog sat down\",\n",
    "    \"the dog ate food\",\n",
    "    \"a cat chased a mouse\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"a dog barked loudly\",\n",
    "    \"the cat meowed softly\",\n",
    "    \"the bird sang sweetly\",\n",
    "    \"a bird flew away\",\n",
    "    \"the fish swam fast\",\n",
    "    \"a fish jumped high\",\n",
    "    \"the boy played ball\",\n",
    "    \"the girl sang song\",\n",
    "    \"a boy read book\",\n",
    "    \"a girl wrote letter\",\n",
    "    \"the sun shines bright\",\n",
    "    \"the moon glows softly\",\n",
    "    \"the stars twinkle bright\",\n",
    "    \"a cat slept quietly\"\n",
    "]\n",
    "\n",
    "# 3. Vocabulary\n",
    "tokens = sorted(list(set(\" \".join(corpus).split())))\n",
    "word2idx = {w: i for i, w in enumerate(tokens)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "V = len(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e717c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Co-occurrence Matrix\n",
    "def build_cooccurrence_matrix(corpus, vocab_size, word2idx, window_size=2):\n",
    "    cooccurrence = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for i, word in enumerate(words):\n",
    "            target_idx = word2idx[word]\n",
    "            for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    context_idx = word2idx[words[j]]\n",
    "                    distance = abs(i - j)\n",
    "                    cooccurrence[target_idx, context_idx] += 1.0 / distance  # Decay based on distance                    \n",
    "    return cooccurrence\n",
    "\n",
    "cooccurrence_matrix = build_cooccurrence_matrix(corpus, V, word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9c08d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the GloVe model: 924\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. GloVe Model\n",
    "class GloVe(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(GloVe, self).__init__()\n",
    "        self.w_embed = nn.Embedding(vocab_size, embedding_dim)  # Word embeddings\n",
    "        self.c_embed = nn.Embedding(vocab_size, embedding_dim)  # Context embeddings\n",
    "        self.w_bias = nn.Embedding(vocab_size, 1)  # Word biases\n",
    "        self.c_bias = nn.Embedding(vocab_size, 1)  # Context biases\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.w_embed.weight)\n",
    "        nn.init.xavier_uniform_(self.c_embed.weight)\n",
    "        nn.init.zeros_(self.w_bias.weight)\n",
    "        nn.init.zeros_(self.c_bias.weight)\n",
    "\n",
    "    def forward(self, i, j, cooccurrence):\n",
    "        w_i = self.w_embed(i)  # [B, D]\n",
    "        w_j = self.c_embed(j)  # [B, D]\n",
    "        b_i = self.w_bias(i).squeeze()  # [B]\n",
    "        b_j = self.c_bias(j).squeeze()  # [B]\n",
    "        x_ij = torch.sum(w_i * w_j, dim=1) + b_i + b_j  # [B]\n",
    "        weights = torch.clamp(cooccurrence / x_max, max=1.0) ** alpha  # [B]\n",
    "        loss = weights * (x_ij - torch.log(cooccurrence + 1e-10)) ** 2  # [B]\n",
    "        return torch.mean(loss)\n",
    "    \n",
    "    def get_embeddings(self, word2idx, idx2word):\n",
    "            \"\"\"\n",
    "            Returns the embeddings as a dictionary mapping words to their embeddings.\n",
    "            \"\"\"\n",
    "            combined_embeddings = self.w_embed.weight.data.cpu().numpy() + self.c_embed.weight.data.cpu().numpy()\n",
    "            embeddings_dict = {idx2word[i]: combined_embeddings[i] for i in range(len(idx2word))}\n",
    "            return embeddings_dict    \n",
    "    \n",
    "    \n",
    "\n",
    "model = GloVe(V, embedding_dim)\n",
    "# Calculate and print the total number of parameters in the model\n",
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "print(f\"Total number of parameters in the GloVe model: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7edd0359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 0.0085\n",
      "Epoch 2/25, Loss: 0.0079\n",
      "Epoch 3/25, Loss: 0.0073\n",
      "Epoch 4/25, Loss: 0.0068\n",
      "Epoch 5/25, Loss: 0.0063\n",
      "Epoch 6/25, Loss: 0.0058\n",
      "Epoch 7/25, Loss: 0.0054\n",
      "Epoch 8/25, Loss: 0.0050\n",
      "Epoch 9/25, Loss: 0.0046\n",
      "Epoch 10/25, Loss: 0.0043\n",
      "Epoch 11/25, Loss: 0.0039\n",
      "Epoch 12/25, Loss: 0.0036\n",
      "Epoch 13/25, Loss: 0.0033\n",
      "Epoch 14/25, Loss: 0.0030\n",
      "Epoch 15/25, Loss: 0.0028\n",
      "Epoch 16/25, Loss: 0.0025\n",
      "Epoch 17/25, Loss: 0.0022\n",
      "Epoch 18/25, Loss: 0.0020\n",
      "Epoch 19/25, Loss: 0.0018\n",
      "Epoch 20/25, Loss: 0.0016\n",
      "Epoch 21/25, Loss: 0.0014\n",
      "Epoch 22/25, Loss: 0.0012\n",
      "Epoch 23/25, Loss: 0.0010\n",
      "Epoch 24/25, Loss: 0.0009\n",
      "Epoch 25/25, Loss: 0.0007\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. Training Loop\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# Convert co-occurrence matrix to sparse format for training\n",
    "i_indices, j_indices = np.nonzero(cooccurrence_matrix)\n",
    "cooccurrences = cooccurrence_matrix[i_indices, j_indices]\n",
    "i_indices = torch.tensor(i_indices, dtype=torch.long)\n",
    "j_indices = torch.tensor(j_indices, dtype=torch.long)\n",
    "cooccurrences = torch.tensor(cooccurrences, dtype=torch.float)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(i_indices, j_indices, cooccurrences)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80c45c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to 'cat': the, away, a, dog, high\n",
      "Top 5 words similar to 'dog': the, away, letter, cat, jumped\n",
      "Top 5 words similar to 'bird': softly, played, quietly, sun, meowed\n",
      "Top 5 words similar to 'fish': book, wrote, barked, slept, jumped\n",
      "Top 5 words similar to 'boy': sun, meowed, shines, stars, read\n"
     ]
    }
   ],
   "source": [
    "# Get the embeddings as a dictionary\n",
    "embeddings = model.get_embeddings(word2idx, idx2word)\n",
    "\n",
    "# Example: Find top similar words for \"cat\"\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_top_similar_words(word, embeddings, top_n=5):\n",
    "    if word not in embeddings:\n",
    "        print(f\"Word '{word}' not in vocabulary.\")\n",
    "        return\n",
    "    word_vec = embeddings[word].reshape(1, -1)\n",
    "    all_words = list(embeddings.keys())\n",
    "    all_vectors = np.array([embeddings[w] for w in all_words])\n",
    "    similarities = cosine_similarity(word_vec, all_vectors).flatten()\n",
    "    similar_indices = similarities.argsort()[::-1][1:top_n + 1]  # Skip the word itself\n",
    "    similar_words = [all_words[i] for i in similar_indices]\n",
    "    print(f\"Top {top_n} words similar to '{word}': {', '.join(similar_words)}\")\n",
    "\n",
    "# Example: Find top similar words for a few words in the vocabulary\n",
    "example_words = [\"cat\", \"dog\", \"bird\", \"fish\", \"boy\"]\n",
    "for word in example_words:\n",
    "    find_top_similar_words(word, embeddings,top_n=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
