{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b1a97cb",
   "metadata": {},
   "source": [
    "## Gradients for cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eeaf70",
   "metadata": {},
   "source": [
    "### Manual Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f142d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ----- Setup -----\n",
    "np.random.seed(0)\n",
    "m, n, k = 5, 4, 3  # examples, features, classes\n",
    "\n",
    "X = np.random.randn(n, m)                        # shape: (n, m)\n",
    "W = np.random.randn(k, n)                        # shape: (k, n)\n",
    "b = np.random.randn(k, 1)                        # shape: (k, 1)\n",
    "Y = np.eye(k)[:, np.random.choice(k, m)]         # shape: (k, m), one-hot labels\n",
    "\n",
    "# ----- Forward pass -----\n",
    "Z = W @ X + b                                    # (k, m)\n",
    "\n",
    "def softmax(Z):\n",
    "    eZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return eZ / np.sum(eZ, axis=0, keepdims=True)\n",
    "\n",
    "A = softmax(Z)                                   # (k, m)\n",
    "loss = -np.sum(Y * np.log(A)) / m\n",
    "\n",
    "# ----- Manual Gradients -----\n",
    "dZ = A - Y                                       # (k, m)\n",
    "dW = (dZ @ X.T) / m                              # (k, n)\n",
    "db = (dZ @ np.ones((m, 1))) / m                  # (k, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35360ba",
   "metadata": {},
   "source": [
    "###  PyTorch (Autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c003d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Tensors\n",
    "X_t = torch.tensor(X.T, dtype=torch.float32)              # (m, n)\n",
    "Y_idx = torch.tensor(np.argmax(Y, axis=0), dtype=torch.long)  # (m,)\n",
    "\n",
    "W_t = torch.tensor(W, dtype=torch.float32, requires_grad=True)  # (k, n)\n",
    "b_t = torch.tensor(b.squeeze(), dtype=torch.float32, requires_grad=True)  # (k,)\n",
    "\n",
    "Z_t = X_t @ W_t.T + b_t                                   # (m, k)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_t = loss_fn(Z_t, Y_idx)\n",
    "loss_t.backward()\n",
    "\n",
    "# Autograd Gradients\n",
    "dW_torch = W_t.grad.detach().numpy()                      # (k, n)\n",
    "db_torch = b_t.grad.detach().numpy().reshape(-1, 1)       # (k, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f03ff",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a8acfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||dW_manual - dW_torch|| = 6.086321339476888e-08\n",
      "||db_manual - db_torch|| = 4.8447627164651547e-08\n"
     ]
    }
   ],
   "source": [
    "print(\"||dW_manual - dW_torch|| =\", np.linalg.norm(dW - dW_torch))\n",
    "print(\"||db_manual - db_torch|| =\", np.linalg.norm(db - db_torch))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
