{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1036452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Skip-Gram with Negative Sampling\n",
    "# =========================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# 1. Hyperparameters\n",
    "embedding_dim = 10\n",
    "context_size = 2   # number of words on each side\n",
    "num_negatives = 5  # number of negative samples\n",
    "epochs = 25\n",
    "lr = 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "948c4690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Toy Corpus (~20 sentences)\n",
    "# -----------------------------\n",
    "corpus = [\n",
    "    \"the cat sat down\",\n",
    "    \"the cat ate food\",\n",
    "    \"the dog sat down\",\n",
    "    \"the dog ate food\",\n",
    "    \"a cat chased a mouse\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"a dog barked loudly\",\n",
    "    \"the cat meowed softly\",\n",
    "    \"the bird sang sweetly\",\n",
    "    \"a bird flew away\",\n",
    "    \"the fish swam fast\",\n",
    "    \"a fish jumped high\",\n",
    "    \"the boy played ball\",\n",
    "    \"the girl sang song\",\n",
    "    \"a boy read book\",\n",
    "    \"a girl wrote letter\",\n",
    "    \"the sun shines bright\",\n",
    "    \"the moon glows softly\",\n",
    "    \"the stars twinkle bright\",\n",
    "    \"a cat slept quietly\"\n",
    "]\n",
    "\n",
    "\n",
    "# 2. Vocabulary\n",
    "tokens = sorted(list(set(\" \".join(corpus).split())))\n",
    "word2idx = {w: i for i, w in enumerate(tokens)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "V = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52300133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word       |    Noise Probability\n",
      "-----------------------------------\n",
      "a          |             0.073365\n",
      "ate        |             0.025938\n",
      "away       |             0.015423\n",
      "ball       |             0.015423\n",
      "barked     |             0.015423\n",
      "bird       |             0.025938\n",
      "book       |             0.015423\n",
      "boy        |             0.025938\n",
      "bright     |             0.025938\n",
      "cat        |             0.059127\n",
      "chased     |             0.025938\n",
      "dog        |             0.043623\n",
      "down       |             0.025938\n",
      "fast       |             0.015423\n",
      "fish       |             0.025938\n",
      "flew       |             0.015423\n",
      "food       |             0.025938\n",
      "girl       |             0.025938\n",
      "glows      |             0.015423\n",
      "high       |             0.015423\n",
      "jumped     |             0.015423\n",
      "letter     |             0.015423\n",
      "loudly     |             0.015423\n",
      "meowed     |             0.015423\n",
      "moon       |             0.015423\n",
      "mouse      |             0.015423\n",
      "played     |             0.015423\n",
      "quietly    |             0.015423\n",
      "read       |             0.015423\n",
      "sang       |             0.025938\n",
      "sat        |             0.025938\n",
      "shines     |             0.015423\n",
      "slept      |             0.015423\n",
      "softly     |             0.025938\n",
      "song       |             0.015423\n",
      "stars      |             0.015423\n",
      "sun        |             0.015423\n",
      "swam       |             0.015423\n",
      "sweetly    |             0.015423\n",
      "the        |             0.111626\n",
      "twinkle    |             0.015423\n",
      "wrote      |             0.015423\n"
     ]
    }
   ],
   "source": [
    "# 3. Noise distribution for negative sampling\n",
    "import collections\n",
    "counts = collections.Counter(\" \".join(corpus).split())\n",
    "total = sum(counts.values())\n",
    "freqs = torch.tensor([counts[w]/total for w in tokens], dtype=torch.float)\n",
    "# Use unigram^3/4 for negative sampling (Mikolov et al.)\n",
    "noise_dist = freqs ** 0.75\n",
    "noise_dist = noise_dist / noise_dist.sum()\n",
    "\n",
    "# Print the noise distribution with words\n",
    "print(f\"{'Word':<10} | {'Noise Probability':>20}\")\n",
    "print(\"-\" * 35)\n",
    "for w, p in zip(tokens, noise_dist):\n",
    "    print(f\"{w:<10} | {p.item():>20.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c041eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Generate Skip-Gram pairs\n",
    "def generate_skipgram_pairs(corpus, context_size=2):\n",
    "    pairs = []\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for i, target in enumerate(words):\n",
    "            target_idx = word2idx[target]\n",
    "            # context window\n",
    "            for j in range(max(0, i - context_size), min(len(words), i + context_size + 1)):\n",
    "                if j != i:\n",
    "                    context_idx = word2idx[words[j]]\n",
    "                    pairs.append((target_idx, context_idx))\n",
    "    return pairs\n",
    "\n",
    "pairs = generate_skipgram_pairs(corpus, context_size)\n",
    "\n",
    "# 5. Dataset\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "\n",
    "dataset = SkipGramDataset(pairs)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66d1bdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center words: ['stars', 'girl', 'glows', 'ate']\n",
      "Context words: ['the', 'sang', 'moon', 'the']\n"
     ]
    }
   ],
   "source": [
    "# Example of iterating through the DataLoader and printing words\n",
    "for center, context in dataloader:\n",
    "    center_words = [idx2word[idx.item()] for idx in center]\n",
    "    context_words = [idx2word[idx.item()] for idx in context]\n",
    "    \n",
    "    print(\"Center words:\", center_words)\n",
    "    print(\"Context words:\", context_words)\n",
    "    break  # Display only the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c14349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Skip-Gram Model\n",
    "\n",
    "class SkipGramNS(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, center, context, neg_samples):\n",
    "        # center: [B]\n",
    "        # context: [B]\n",
    "        # neg_samples: [B, K]\n",
    "        v_c = self.in_embed(center)           # [B, D]\n",
    "        u_o = self.out_embed(context)         # [B, D]\n",
    "        u_k = self.out_embed(neg_samples)     # [B, K, D]\n",
    "\n",
    "        # positive score\n",
    "        pos_score = torch.sum(v_c * u_o, dim=1)  # [B]\n",
    "        pos_loss = F.logsigmoid(pos_score)\n",
    "\n",
    "        # negative score\n",
    "        neg_score = torch.bmm(u_k, v_c.unsqueeze(2)).squeeze()  # [B, K]\n",
    "        neg_loss = F.logsigmoid(-neg_score).sum(1)             # [B]\n",
    "\n",
    "        return -(pos_loss + neg_loss).mean()  # mean over batch\n",
    "\n",
    "model = SkipGramNS(V, embedding_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a6bb591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Parameters ===\n",
      "in_embed.weight      |        420 params\n",
      "out_embed.weight     |        420 params\n",
      "-------------------------------------------------------\n",
      "Total Trainable Params: 840\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    print(\"\\n=== Model Parameters ===\")\n",
    "    total_params = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            n_params = param.numel()\n",
    "            mem = n_params * param.element_size()  # bytes\n",
    "            total_params += n_params\n",
    "            print(f\"{name:20s} | {n_params:10,d} params\")\n",
    "\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"Total Trainable Params: {total_params:,}\")\n",
    "\n",
    "count_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f137462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 409.4568\n",
      "Epoch 2/25, Loss: 360.4789\n",
      "Epoch 3/25, Loss: 329.2030\n",
      "Epoch 4/25, Loss: 280.4951\n",
      "Epoch 5/25, Loss: 255.1034\n",
      "Epoch 6/25, Loss: 227.7789\n",
      "Epoch 7/25, Loss: 215.8456\n",
      "Epoch 8/25, Loss: 194.4384\n",
      "Epoch 9/25, Loss: 178.8957\n",
      "Epoch 10/25, Loss: 180.0030\n",
      "Epoch 11/25, Loss: 155.6350\n",
      "Epoch 12/25, Loss: 159.0434\n",
      "Epoch 13/25, Loss: 145.7386\n",
      "Epoch 14/25, Loss: 139.7112\n",
      "Epoch 15/25, Loss: 134.7986\n",
      "Epoch 16/25, Loss: 125.8668\n",
      "Epoch 17/25, Loss: 116.9548\n",
      "Epoch 18/25, Loss: 122.2632\n",
      "Epoch 19/25, Loss: 116.8848\n",
      "Epoch 20/25, Loss: 112.1864\n",
      "Epoch 21/25, Loss: 110.2257\n",
      "Epoch 22/25, Loss: 111.3834\n",
      "Epoch 23/25, Loss: 107.0279\n",
      "Epoch 24/25, Loss: 105.1920\n",
      "Epoch 25/25, Loss: 106.6072\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7. Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for center, context in dataloader:\n",
    "        # generate negative samples\n",
    "        neg_samples = torch.multinomial(noise_dist, len(center)*num_negatives, replacement=True)\n",
    "        neg_samples = neg_samples.view(len(center), num_negatives)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(center, context, neg_samples)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b345d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 dot-product neighbors for 'cat':\n",
      "Word         |  Dot-Product\n",
      "------------------------------\n",
      "slept        |      -0.5227\n",
      "quietly      |      -0.6172\n",
      "the          |      -0.8214\n",
      "chased       |      -1.1401\n",
      "meowed       |      -1.2194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def predict_top_context_dot(center_word, top_k=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # index of the center word\n",
    "        center_idx = torch.tensor([word2idx[center_word]])\n",
    "\n",
    "        # input embedding of center word  [1, D]\n",
    "        v_c = model.in_embed(center_idx)\n",
    "\n",
    "        # all output embeddings  [V, D]\n",
    "        u_all = model.out_embed.weight\n",
    "\n",
    "        # ---- DOT PRODUCT SCORES (SGNS objective) ----\n",
    "        # score = v_c @ u_o^T\n",
    "        scores = torch.matmul(u_all, v_c.t()).squeeze()     # [V]\n",
    "\n",
    "        # exclude center word itself\n",
    "        scores[word2idx[center_word]] = -float('inf')\n",
    "\n",
    "        # get top-k\n",
    "        topk_scores, topk_idx = torch.topk(scores, top_k)\n",
    "\n",
    "        top_words = [idx2word[i.item()] for i in topk_idx]\n",
    "        top_scores = [s.item() for s in topk_scores]\n",
    "\n",
    "        print(f\"\\nTop {top_k} dot-product neighbors for '{center_word}':\")\n",
    "        print(f\"{'Word':<12} | {'Dot-Product':>12}\")\n",
    "        print(\"-\" * 30)\n",
    "        for w, s in zip(top_words, top_scores):\n",
    "            print(f\"{w:<12} | {s:>12.4f}\")\n",
    "        print()\n",
    "\n",
    "        return list(zip(top_words, top_scores))\n",
    "\n",
    "\n",
    "# Example\n",
    "center = \"cat\"\n",
    "_ = predict_top_context_dot(center, top_k=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
