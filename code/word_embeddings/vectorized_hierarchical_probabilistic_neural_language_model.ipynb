{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f872da38",
   "metadata": {},
   "source": [
    "### Vectorized implementation - Hierarchical Probabilistic Neural Network Language Model, Morin & Y Bengio (2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c197405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Hierarchical Probabilistic Neural \n",
    "# Network Language Model, \n",
    "# Morin & Bengio (2005) \n",
    "# =========================\n",
    "\n",
    "# 1. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 2. Config\n",
    "n = 4        # context size = n-1 previous words + 1 target\n",
    "m = 10       # embedding dimension of each word [m x 1]\n",
    "h = 16       # hidden layer dimension [h x 1]\n",
    "d_node = 16  # dimension of each node in the tree [d_node x 1]\n",
    "\n",
    "epochs = 25\n",
    "lr = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c8e88",
   "metadata": {},
   "source": [
    "#### Toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d03db988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['a', 'ate', 'away', 'ball', 'barked', 'bird', 'book', 'boy', 'bright', 'cat', 'chased', 'dog', 'down', 'fast', 'fish', 'flew', 'food', 'girl', 'glows', 'high', 'jumped', 'letter', 'loudly', 'meowed', 'moon', 'mouse', 'played', 'quietly', 'read', 'sang', 'sat', 'shines', 'slept', 'softly', 'song', 'stars', 'sun', 'swam', 'sweetly', 'the', 'twinkle', 'wrote']\n",
      "Vocabulary Size: 42\n",
      "Vocabulary size: 42\n",
      "Number of training samples: 22\n",
      "Example context: [39, 9, 30] -> target: 12\n",
      "Example context words: ['the', 'cat', 'sat']\n",
      "Example target word: down\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 3. Toy Corpus (~20 sentences)\n",
    "# -----------------------------\n",
    "corpus = [\n",
    "    \"the cat sat down\",\n",
    "    \"the cat ate food\",\n",
    "    \"the dog sat down\",\n",
    "    \"the dog ate food\",\n",
    "    \"a cat chased a mouse\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"a dog barked loudly\",\n",
    "    \"the cat meowed softly\",\n",
    "    \"the bird sang sweetly\",\n",
    "    \"a bird flew away\",\n",
    "    \"the fish swam fast\",\n",
    "    \"a fish jumped high\",\n",
    "    \"the boy played ball\",\n",
    "    \"the girl sang song\",\n",
    "    \"a boy read book\",\n",
    "    \"a girl wrote letter\",\n",
    "    \"the sun shines bright\",\n",
    "    \"the moon glows softly\",\n",
    "    \"the stars twinkle bright\",\n",
    "    \"a cat slept quietly\"\n",
    "]\n",
    "\n",
    "words = sorted(set(\" \".join(corpus).split()))\n",
    "\n",
    "print(\"Vocabulary:\", words)\n",
    "print(\"Vocabulary Size:\", len(words))\n",
    "\n",
    "# 4. Preprocessing\n",
    "tokens = set(\" \".join(corpus).split())\n",
    "word2idx = {word: i for i, word in enumerate(sorted(tokens))}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "V = len(word2idx)   # vocabulary size |V|\n",
    "\n",
    "# make context-target pairs for n-gram model\n",
    "def make_ngrams(corpus, n):\n",
    "    X, y = [], []\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words) - n):\n",
    "            context = words[i:i+n]\n",
    "            target = words[i+n]\n",
    "            X.append([word2idx[w] for w in context])\n",
    "            y.append(word2idx[target])\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "X, y = make_ngrams(corpus, n-1)\n",
    "\n",
    "\n",
    "print(\"Vocabulary size:\", V)\n",
    "print(\"Number of training samples:\", len(X))\n",
    "print('Example context:', X[0].tolist(), '-> target:', y[0].item())\n",
    "print('Example context words:', [idx2word[i] for i in X[0].tolist()])\n",
    "print('Example target word:', idx2word[y[0].item()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd51b29",
   "metadata": {},
   "source": [
    "#### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0f37161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Dataset/Dataloader\n",
    "class NGramDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(NGramDataset(X, y), batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce2960",
   "metadata": {},
   "source": [
    "#### Forming the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a41dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_strict_balanced(words, d):\n",
    "    counter = [0]  # Mutable counter for naming internal nodes\n",
    "    \n",
    "    def build_recursive(words):\n",
    "        if not words:\n",
    "            return None\n",
    "        if len(words) == 1:\n",
    "            return {\"name\": words[0]}  # Leaf node - no parameters needed\n",
    "            \n",
    "        # Internal node with parameters\n",
    "        node_name = f\"n{counter[0]}\"\n",
    "        counter[0] += 1\n",
    "        \n",
    "        mid = len(words)//2\n",
    "        node = {\n",
    "            \"name\": node_name,\n",
    "            \"left\": build_recursive(words[:mid]),\n",
    "            \"right\": build_recursive(words[mid:])\n",
    "        }\n",
    "        return node\n",
    "    \n",
    "    return build_recursive(words)\n",
    "\n",
    "# Create balanced tree with parameters\n",
    "balanced_tree = build_strict_balanced(words, d_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17242a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tree(tree, prefix=\"\", paths=None, depths=None, freqs=None):\n",
    "    \"\"\"\n",
    "    Analyze tree paths, depths and frequencies recursively\n",
    "    If freqs is None, uses weight=1 for each word\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries\n",
    "    paths = {} if paths is None else paths\n",
    "    depths = {} if depths is None else depths\n",
    "    freqs = {} if freqs is None else freqs\n",
    "    \n",
    "    # If leaf node, record path, depth and frequency\n",
    "    if not tree.get('left') and not tree.get('right'):\n",
    "        word = tree['name']\n",
    "        freq = tree.get('freq', 1)  # Default frequency = 1 if not specified\n",
    "        paths[word] = prefix\n",
    "        depths[word] = len(prefix)\n",
    "        freqs[word] = freq\n",
    "        return paths, depths, freqs\n",
    "    \n",
    "    # Recurse on children\n",
    "    if tree.get('left'):\n",
    "        analyze_tree(tree['left'], prefix + \"0\", paths, depths, freqs)\n",
    "    if tree.get('right'):\n",
    "        analyze_tree(tree['right'], prefix + \"1\", paths, depths, freqs)\n",
    "    \n",
    "    return paths, depths, freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af97c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tree(tree, words):\n",
    "    \"\"\"\n",
    "    Converts tree into tensors for vectorized HierarchicalSoftmaxLM.\n",
    "    Returns: path_nodes, path_bits, num_nodes, unk_node_id, word2idx\n",
    "    \"\"\"\n",
    "    node2id = {}\n",
    "    counter = [0]\n",
    "\n",
    "    # Assign IDs only to internal nodes\n",
    "    def assign_ids(node):\n",
    "        if not node.get(\"left\") and not node.get(\"right\"):\n",
    "            return\n",
    "        node2id[node[\"name\"]] = counter[0]\n",
    "        counter[0] += 1\n",
    "        if node.get(\"left\"):  assign_ids(node[\"left\"])\n",
    "        if node.get(\"right\"): assign_ids(node[\"right\"])\n",
    "\n",
    "    assign_ids(tree)\n",
    "    num_nodes = counter[0]\n",
    "\n",
    "    # Special UNK node for padding\n",
    "    unk_node_id = num_nodes\n",
    "    num_nodes += 1  # include UNK\n",
    "\n",
    "    # Get bit paths as strings\n",
    "    paths, depths, freqs = analyze_tree(tree)\n",
    "\n",
    "    # Build word→idx\n",
    "    word2idx = {w: i for i, w in enumerate(words)}\n",
    "\n",
    "    # Convert paths to node/bit sequences\n",
    "    max_len = max(len(p) for p in paths.values())\n",
    "    path_nodes = torch.full((len(words), max_len), fill_value=unk_node_id, dtype=torch.long)\n",
    "    path_bits  = torch.full((len(words), max_len), fill_value=-1, dtype=torch.long)\n",
    "\n",
    "    def traverse_for_nodes(node, prefix=\"\"):\n",
    "        if not node.get(\"left\") and not node.get(\"right\"):\n",
    "            word = node[\"name\"]\n",
    "            wid = word2idx[word]\n",
    "            for i, bit in enumerate(prefix):\n",
    "                # find the node ID for this decision\n",
    "                # step through prefix until this depth\n",
    "                curr = tree\n",
    "                for b in prefix[:i]:\n",
    "                    curr = curr[\"left\"] if b == \"0\" else curr[\"right\"]\n",
    "                nid = node2id[curr[\"name\"]]\n",
    "                path_nodes[wid, i] = nid\n",
    "                path_bits[wid, i] = int(bit)\n",
    "            return\n",
    "        if node.get(\"left\"):  traverse_for_nodes(node[\"left\"], prefix + \"0\")\n",
    "        if node.get(\"right\"): traverse_for_nodes(node[\"right\"], prefix + \"1\")\n",
    "\n",
    "    traverse_for_nodes(tree)\n",
    "\n",
    "    return path_nodes, path_bits, num_nodes, unk_node_id, word2idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d419eee8",
   "metadata": {},
   "source": [
    "#### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc4fa04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HierarchicalSoftmaxLM(nn.Module):\n",
    "    def __init__(self, vocab_size, n, m, h, d_node,\n",
    "                 word2idx, path_nodes, path_bits, num_nodes, unk_node_id):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.h = h\n",
    "        self.d_node = d_node\n",
    "        self.word2idx = word2idx\n",
    "        self.path_nodes = path_nodes      # [vocab_size, L]\n",
    "        self.path_bits = path_bits        # [vocab_size, L]\n",
    "        self.num_nodes = num_nodes\n",
    "        self.unk_node_id = unk_node_id\n",
    "        self.max_path_len = path_nodes.size(1)\n",
    "\n",
    "        # word embeddings\n",
    "        self.embeddings = nn.Embedding(vocab_size, m)\n",
    "\n",
    "        # shared transforms\n",
    "        self.W = nn.Linear((n-1)*m, h, bias=False)   # context → hidden\n",
    "        self.U = nn.Linear(d_node, h, bias=False)    # node embedding → hidden\n",
    "        self.c = nn.Parameter(torch.zeros(h))\n",
    "        self.beta = nn.Parameter(torch.randn(h))\n",
    "\n",
    "        # node parameters (vectorized)\n",
    "        self.N = nn.Embedding(num_nodes, d_node)   # node embeddings\n",
    "        self.alpha = nn.Embedding(num_nodes, 1)    # node biases\n",
    "\n",
    "    def forward(self, context_idxs, target_idxs):\n",
    "        B = context_idxs.size(0)\n",
    "\n",
    "        # ===== Context representation x =====\n",
    "        ctx_emb = self.embeddings(context_idxs)      # [B, n-1, m]\n",
    "        x = ctx_emb.view(B, -1)                      # [B, (n-1)*m]\n",
    "\n",
    "        # ===== Path lookup =====\n",
    "        path_nodes_batch = self.path_nodes[target_idxs]   # [B, L]\n",
    "        path_bits_batch  = self.path_bits[target_idxs]    # [B, L]\n",
    "\n",
    "        # Node embeddings & biases for this batch\n",
    "        N_paths    = self.N(path_nodes_batch)        # [B, L, d_node]\n",
    "        alpha_path = self.alpha(path_nodes_batch)    # [B, L, 1]\n",
    "\n",
    "        # ===== Hidden computation =====\n",
    "        h_x = self.W(x)                              # [B, h]\n",
    "        h_N = self.U(N_paths)                        # [B, L, h]\n",
    "        h = torch.tanh(h_x.unsqueeze(1) + h_N + self.c)   # [B, L, h]\n",
    "\n",
    "        # ===== Probabilities =====\n",
    "        p = torch.sigmoid(alpha_path.squeeze(-1) + (h @ self.beta))  # [B, L]\n",
    "\n",
    "        # ===== Loss =====\n",
    "        mask = (path_bits_batch != -1)    # ignore padding\n",
    "        target = path_bits_batch.float()\n",
    "        loss = F.binary_cross_entropy(p, target, reduction=\"none\")\n",
    "\n",
    "        # [B, L]\n",
    "        masked_loss = loss * mask\n",
    "\n",
    "        # average over valid path positions for each sample\n",
    "        loss_per_sample = masked_loss.sum(dim=1) / mask.sum(dim=1)   # [B]\n",
    "\n",
    "        # mean over batch\n",
    "        loss = loss_per_sample.mean()\n",
    "\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def forward_naive(self, context_idxs, target_idxs):\n",
    "        \"\"\"\n",
    "        Naive forward: loop over batch and over path positions.\n",
    "        Equivalent to vectorized forward(), useful for debugging.\n",
    "        \"\"\"\n",
    "        B = context_idxs.size(0)\n",
    "        \n",
    "        # ===== Context representation x =====\n",
    "        ctx_emb = self.embeddings(context_idxs)      # [B, n-1, m]\n",
    "        x = ctx_emb.view(B, -1)                      # [B, (n-1)*m]\n",
    "\n",
    "        losses = []\n",
    "        for i in range(B):\n",
    "            # one sample in the batch\n",
    "            target_idx = target_idxs[i]\n",
    "            path_nodes_i = self.path_nodes[target_idx]   # [L]\n",
    "            path_bits_i  = self.path_bits[target_idx]    # [L]\n",
    "\n",
    "            h_x = self.W(x[i].unsqueeze(0))  # [1, h]\n",
    "            loss_i = 0.0\n",
    "            count = 0\n",
    "\n",
    "            for j, node_id in enumerate(path_nodes_i):\n",
    "                if path_bits_i[j] == -1:  # padding\n",
    "                    continue\n",
    "\n",
    "                # node parameters\n",
    "                N_j = self.N(node_id)           # [d_node]\n",
    "                alpha_j = self.alpha(node_id)   # [1]\n",
    "\n",
    "                # hidden\n",
    "                h_N = self.U(N_j.unsqueeze(0))  # [1, h]\n",
    "                h = torch.tanh(h_x + h_N + self.c)  # [1, h]\n",
    "\n",
    "                # prob\n",
    "                p = torch.sigmoid(alpha_j + (h @ self.beta))  # [1]\n",
    "                \n",
    "                # target bit\n",
    "                b_val = path_bits_i[j].float()\n",
    "                loss_j = -(b_val * torch.log(p + 1e-9) +\n",
    "                           (1 - b_val) * torch.log(1 - p + 1e-9))\n",
    "                loss_i += loss_j\n",
    "                count += 1\n",
    "\n",
    "            if count > 0:\n",
    "                loss_i = loss_i / count\n",
    "            losses.append(loss_i)\n",
    "\n",
    "        return torch.stack(losses).mean()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_next(self, context_idxs):\n",
    "        \"\"\"\n",
    "        Vectorized next-word probability computation using hierarchical softmax.\n",
    "        Returns [B, V] probabilities.\n",
    "        \"\"\"\n",
    "        B = context_idxs.size(0)\n",
    "\n",
    "        # ===== Context representation =====\n",
    "        ctx_emb = self.embeddings(context_idxs)  # [B, n-1, m]\n",
    "        x = ctx_emb.view(B, -1)                 # [B, (n-1)*m]\n",
    "        h_x = self.W(x)                         # [B, h]\n",
    "\n",
    "        # Expand across all vocab words\n",
    "        path_nodes_batch = self.path_nodes.unsqueeze(0).expand(B, -1, -1)  # [B, V, L]\n",
    "        path_bits_batch  = self.path_bits.unsqueeze(0).expand(B, -1, -1)   # [B, V, L]\n",
    "\n",
    "        # Node params\n",
    "        N_paths    = self.N(path_nodes_batch)     # [B, V, L, d_node]\n",
    "        alpha_path = self.alpha(path_nodes_batch) # [B, V, L, 1]\n",
    "\n",
    "        # Hidden computation\n",
    "        h_N = self.U(N_paths)  # [B, V, L, h]\n",
    "        h   = torch.tanh(h_x.unsqueeze(1).unsqueeze(2) + h_N + self.c)  # [B, V, L, h]\n",
    "\n",
    "        # Node probabilities\n",
    "        p = torch.sigmoid(alpha_path.squeeze(-1) + (h @ self.beta))  # [B, V, L]\n",
    "\n",
    "        # Mask padding\n",
    "        mask = (path_bits_batch != -1).float()  # [B, V, L]\n",
    "        bits = path_bits_batch.float()\n",
    "\n",
    "        # Log-prob along path\n",
    "        logp = bits * torch.log(p + 1e-9) + (1 - bits) * torch.log(1 - p + 1e-9)\n",
    "        logp = (logp * mask).sum(dim=-1)  # sum over path length → [B, V]\n",
    "\n",
    "        probs = torch.exp(logp)  # true hierarchical softmax probabilities\n",
    "        return probs  # [B, V]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_next_naive(self, context_idxs):\n",
    "        \"\"\"\n",
    "        Naive loop-based next-word probability computation.\n",
    "        \"\"\"\n",
    "        B = context_idxs.size(0)\n",
    "\n",
    "        # Context representation\n",
    "        ctx_emb = self.embeddings(context_idxs)   # [B, n-1, m]\n",
    "        x = ctx_emb.view(B, -1)                   # [B, (n-1)*m]\n",
    "\n",
    "        probs = []\n",
    "        for i in range(B):\n",
    "            h_x = self.W(x[i].unsqueeze(0))  # [1, h]\n",
    "            word_probs = []\n",
    "\n",
    "            for target_idx in range(self.vocab_size):\n",
    "                path_nodes_i = self.path_nodes[target_idx]  # [L]\n",
    "                path_bits_i  = self.path_bits[target_idx]   # [L]\n",
    "\n",
    "                logp = 0.0\n",
    "                for j, node_id in enumerate(path_nodes_i):\n",
    "                    if path_bits_i[j] == -1:  # padding\n",
    "                        continue\n",
    "\n",
    "                    N_j = self.N(node_id)\n",
    "                    alpha_j = self.alpha(node_id)\n",
    "\n",
    "                    h_N = self.U(N_j.unsqueeze(0))  # [1, h]\n",
    "                    h = torch.tanh(h_x + h_N + self.c)  # [1, h]\n",
    "\n",
    "                    p = torch.sigmoid(alpha_j + (h @ self.beta))\n",
    "                    b_val = float(path_bits_i[j])\n",
    "                    logp += b_val * torch.log(p + 1e-9) + (1 - b_val) * torch.log(1 - p + 1e-9)\n",
    "\n",
    "                word_probs.append(logp)\n",
    "\n",
    "            logps = torch.stack(word_probs).squeeze()  # [V]\n",
    "            probs.append(torch.exp(logps))            # convert log → prob\n",
    "\n",
    "        probs = torch.stack(probs, dim=0)  # [B, V]\n",
    "        return probs    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33735bb7",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86583025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1902\n",
      "c                              [16]                 16\n",
      "beta                           [16]                 16\n",
      "embeddings.weight              [42, 10]             420\n",
      "W.weight                       [16, 30]             480\n",
      "U.weight                       [16, 16]             256\n",
      "N.weight                       [42, 16]             672\n",
      "alpha.weight                   [42, 1]              42\n",
      "Total                                               1902\n"
     ]
    }
   ],
   "source": [
    "balanced_tree = build_strict_balanced(words, d_node)\n",
    "path_nodes, path_bits, num_nodes, unk_node_id, word2idx = preprocess_tree(balanced_tree, words)\n",
    "\n",
    "model = HierarchicalSoftmaxLM(\n",
    "    vocab_size=len(words),\n",
    "    n=n, m=m, h=h, d_node=d_node,\n",
    "    word2idx=word2idx,\n",
    "    path_nodes=path_nodes,\n",
    "    path_bits=path_bits,\n",
    "    num_nodes=num_nodes,\n",
    "    unk_node_id=unk_node_id\n",
    ")\n",
    "\n",
    "# Print model architecture and parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters:\", count_parameters(model))\n",
    "\n",
    "\n",
    "def parameter_report(model):\n",
    "    report = []\n",
    "    total = 0\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            num = p.numel()\n",
    "            total += num\n",
    "            report.append(f\"{name:30} {str(list(p.shape)):20} {num}\")\n",
    "    report.append(f\"{'Total':30} {'':20} {total}\")\n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "print(parameter_report(model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7756ca",
   "metadata": {},
   "source": [
    "#### Comparing the naive and vectorized implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb700a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss: 0.8713, Vectorized loss: 0.8713\n",
      "Naive loss: 0.9398, Vectorized loss: 0.9398\n",
      "Naive loss: 1.0094, Vectorized loss: 1.0094\n",
      "Naive loss: 1.5997, Vectorized loss: 1.5997\n",
      "Naive loss: 1.3727, Vectorized loss: 1.3727\n",
      "Naive loss: 1.5239, Vectorized loss: 1.5239\n",
      "Naive loss (Sum):       7.316938698291779\n",
      "Vectorized loss (Sum): 7.316938281059265\n",
      "Difference:      4.172325134277344e-07\n"
     ]
    }
   ],
   "source": [
    "losses_naive = 0\n",
    "losses_vec = 0\n",
    "for context_idxs, target_idxs in train_loader:\n",
    "    with torch.no_grad():\n",
    "        # Naive loss\n",
    "        loss_naive = model.forward_naive(context_idxs, target_idxs)\n",
    "        # Vectorized loss\n",
    "        loss_vec = model.forward(context_idxs, target_idxs)\n",
    "\n",
    "        losses_naive += loss_naive.item()\n",
    "        losses_vec += loss_vec.item()\n",
    "        print(f\"Naive loss: {loss_naive.item():.4f}, Vectorized loss: {loss_vec.item():.4f}\")\n",
    "\n",
    "print(\"Naive loss (Sum):      \", losses_naive)\n",
    "print(\"Vectorized loss (Sum):\", losses_vec)\n",
    "print(\"Difference:     \", abs(losses_naive - losses_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c88764",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0646b0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.1841\n",
      "Epoch 2: Loss = 0.8585\n",
      "Epoch 3: Loss = 0.7073\n",
      "Epoch 4: Loss = 0.6045\n",
      "Epoch 5: Loss = 0.5201\n",
      "Epoch 6: Loss = 0.4887\n",
      "Epoch 7: Loss = 0.4365\n",
      "Epoch 8: Loss = 0.3897\n",
      "Epoch 9: Loss = 0.3371\n",
      "Epoch 10: Loss = 0.2944\n",
      "Epoch 11: Loss = 0.2619\n",
      "Epoch 12: Loss = 0.2294\n",
      "Epoch 13: Loss = 0.2022\n",
      "Epoch 14: Loss = 0.1779\n",
      "Epoch 15: Loss = 0.1547\n",
      "Epoch 16: Loss = 0.1382\n",
      "Epoch 17: Loss = 0.1211\n",
      "Epoch 18: Loss = 0.1058\n",
      "Epoch 19: Loss = 0.0965\n",
      "Epoch 20: Loss = 0.0849\n",
      "Epoch 21: Loss = 0.0760\n",
      "Epoch 22: Loss = 0.0683\n",
      "Epoch 23: Loss = 0.0625\n",
      "Epoch 24: Loss = 0.0570\n",
      "Epoch 25: Loss = 0.0507\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Training\n",
    "# =============================\n",
    "\n",
    "device = torch.device(\"cpu\")  # force CPU for small data\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in train_loader:\n",
    "        context, target = context.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(context, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214ecabf",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29c854bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_words_batch(model, context_words_batch, word2idx, idx2word, k=5, method=\"vectorized\"):\n",
    "    \"\"\"\n",
    "    Predict next word probabilities for a batch of contexts using hierarchical softmax.\n",
    "    \n",
    "    Args:\n",
    "        model: HierarchicalSoftmaxLM instance\n",
    "        context_words_batch: List of contexts, where each context is a list of n-1 words\n",
    "        word2idx: dict mapping word -> index\n",
    "        idx2word: dict mapping index -> word\n",
    "        k: number of top predictions per context\n",
    "        method: \"vectorized\" or \"naive\"\n",
    "    \n",
    "    Returns:\n",
    "        List of lists, where each inner list contains (word, probability) tuples for one context\n",
    "    \"\"\"\n",
    "    # Convert all contexts to indices\n",
    "    batch_size = len(context_words_batch)\n",
    "    context_idxs = torch.tensor([\n",
    "        [word2idx.get(w, model.unk_node_id) for w in context]\n",
    "        for context in context_words_batch\n",
    "    ], dtype=torch.long)  # [B, n-1]\n",
    "\n",
    "    # Get probabilities for all contexts at once\n",
    "    if method == \"vectorized\":\n",
    "        probs = model.predict_next(context_idxs)  # [B, V]\n",
    "    elif method == \"naive\":\n",
    "        probs = model.predict_next_naive(context_idxs)  # [B, V]\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'vectorized' or 'naive'\")\n",
    "\n",
    "    # Get top-k predictions for each context\n",
    "    topk_probs, topk_idx = torch.topk(probs, k, dim=1)  # [B, k]\n",
    "    \n",
    "    # Convert to list of word-probability pairs for each context\n",
    "    results = []\n",
    "    for i in range(batch_size):\n",
    "        context_results = [\n",
    "            (idx2word[idx.item()], prob.item())\n",
    "            for idx, prob in zip(topk_idx[i], topk_probs[i])\n",
    "        ]\n",
    "        results.append(context_results)\n",
    "\n",
    "        # Print predictions for this context\n",
    "        print(f\"\\nContext: {context_words_batch[i]}\")\n",
    "        print(f\"{method.capitalize()} top-{k}:\")\n",
    "        for j, (word, prob) in enumerate(context_results):\n",
    "            print(f\"  {j}. '{word}' ({prob:.4f})\")\n",
    "        print(\"-\" * 50)        \n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e90aa398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "test_contexts = [\n",
    "    [\"the\", \"cat\", \"sat\"],\n",
    "    [\"a\", \"cat\", \"chased\"],\n",
    "    [\"a\", \"dog\", \"barked\"],\n",
    "    [\"the\", \"stars\", \"glows\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61f70080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: ['the', 'cat', 'sat']\n",
      "Naive top-5:\n",
      "  0. 'down' (0.7875)\n",
      "  1. 'away' (0.0666)\n",
      "  2. 'bird' (0.0518)\n",
      "  3. 'a' (0.0245)\n",
      "  4. 'food' (0.0244)\n",
      "--------------------------------------------------\n",
      "\n",
      "Context: ['a', 'cat', 'chased']\n",
      "Naive top-5:\n",
      "  0. 'a' (0.8077)\n",
      "  1. 'away' (0.0612)\n",
      "  2. 'down' (0.0409)\n",
      "  3. 'letter' (0.0313)\n",
      "  4. 'bird' (0.0180)\n",
      "--------------------------------------------------\n",
      "\n",
      "Context: ['a', 'dog', 'barked']\n",
      "Naive top-5:\n",
      "  0. 'loudly' (0.7393)\n",
      "  1. 'food' (0.0655)\n",
      "  2. 'letter' (0.0509)\n",
      "  3. 'mouse' (0.0385)\n",
      "  4. 'quietly' (0.0381)\n",
      "--------------------------------------------------\n",
      "\n",
      "Context: ['the', 'stars', 'glows']\n",
      "Naive top-5:\n",
      "  0. 'food' (0.3163)\n",
      "  1. 'bright' (0.1905)\n",
      "  2. 'book' (0.1835)\n",
      "  3. 'high' (0.0573)\n",
      "  4. 'cat' (0.0474)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get predictions - naive\n",
    "predictions_naive = predict_next_words_batch(model, test_contexts, word2idx, idx2word, k=5, method=\"naive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83c80e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: ['the', 'cat', 'sat']\n",
      "Vectorized top-5:\n",
      "  0. 'down' (0.7875)\n",
      "  1. 'away' (0.0666)\n",
      "  2. 'bird' (0.0518)\n",
      "  3. 'a' (0.0245)\n",
      "  4. 'food' (0.0244)\n",
      "--------------------------------------------------\n",
      "\n",
      "Context: ['a', 'cat', 'chased']\n",
      "Vectorized top-5:\n",
      "  0. 'a' (0.8077)\n",
      "  1. 'away' (0.0612)\n",
      "  2. 'down' (0.0409)\n",
      "  3. 'letter' (0.0313)\n",
      "  4. 'bird' (0.0180)\n",
      "--------------------------------------------------\n",
      "\n",
      "Context: ['a', 'dog', 'barked']\n",
      "Vectorized top-5:\n",
      "  0. 'loudly' (0.7393)\n",
      "  1. 'food' (0.0655)\n",
      "  2. 'letter' (0.0509)\n",
      "  3. 'mouse' (0.0385)\n",
      "  4. 'quietly' (0.0381)\n",
      "--------------------------------------------------\n",
      "\n",
      "Context: ['the', 'stars', 'glows']\n",
      "Vectorized top-5:\n",
      "  0. 'food' (0.3163)\n",
      "  1. 'bright' (0.1905)\n",
      "  2. 'book' (0.1835)\n",
      "  3. 'high' (0.0573)\n",
      "  4. 'cat' (0.0474)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get predictions - vectorized\n",
    "predictions_vectorized = predict_next_words_batch(model, test_contexts, word2idx, idx2word, k=5, method=\"vectorized\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
