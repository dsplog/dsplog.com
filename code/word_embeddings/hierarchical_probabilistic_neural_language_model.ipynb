{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f872da38",
   "metadata": {},
   "source": [
    "### Hierarchical Probabilistic Neural Network Language Model, Morin & Y Bengio (2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c197405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Hierarchical Probabilistic Neural \n",
    "# Network Language Model, \n",
    "# Morin & Bengio (2005) \n",
    "# =========================\n",
    "\n",
    "# 1. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 2. Config\n",
    "n = 4        # context size = n-1 previous words + 1 target\n",
    "m = 10       # embedding dimension of each word [m x 1]\n",
    "h = 16       # hidden layer dimension [h x 1]\n",
    "d_node = 16  # dimension of each node in the tree [d_node x 1]\n",
    "\n",
    "epochs = 25\n",
    "lr = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c8e88",
   "metadata": {},
   "source": [
    "#### Toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d03db988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['a', 'ate', 'away', 'ball', 'barked', 'bird', 'book', 'boy', 'bright', 'cat', 'chased', 'dog', 'down', 'fast', 'fish', 'flew', 'food', 'girl', 'glows', 'high', 'jumped', 'letter', 'loudly', 'meowed', 'moon', 'mouse', 'played', 'quietly', 'read', 'sang', 'sat', 'shines', 'slept', 'softly', 'song', 'stars', 'sun', 'swam', 'sweetly', 'the', 'twinkle', 'wrote']\n",
      "Vocabulary Size: 42\n",
      "Vocabulary size: 42\n",
      "Number of training samples: 22\n",
      "Example context: [39, 9, 30] -> target: 12\n",
      "Example context words: ['the', 'cat', 'sat']\n",
      "Example target word: down\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 3. Toy Corpus (~20 sentences)\n",
    "# -----------------------------\n",
    "corpus = [\n",
    "    \"the cat sat down\",\n",
    "    \"the cat ate food\",\n",
    "    \"the dog sat down\",\n",
    "    \"the dog ate food\",\n",
    "    \"a cat chased a mouse\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"a dog barked loudly\",\n",
    "    \"the cat meowed softly\",\n",
    "    \"the bird sang sweetly\",\n",
    "    \"a bird flew away\",\n",
    "    \"the fish swam fast\",\n",
    "    \"a fish jumped high\",\n",
    "    \"the boy played ball\",\n",
    "    \"the girl sang song\",\n",
    "    \"a boy read book\",\n",
    "    \"a girl wrote letter\",\n",
    "    \"the sun shines bright\",\n",
    "    \"the moon glows softly\",\n",
    "    \"the stars twinkle bright\",\n",
    "    \"a cat slept quietly\"\n",
    "]\n",
    "\n",
    "words = sorted(set(\" \".join(corpus).split()))\n",
    "\n",
    "print(\"Vocabulary:\", words)\n",
    "print(\"Vocabulary Size:\", len(words))\n",
    "\n",
    "# 4. Preprocessing\n",
    "tokens = set(\" \".join(corpus).split())\n",
    "word2idx = {word: i for i, word in enumerate(sorted(tokens))}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "V = len(word2idx)   # vocabulary size |V|\n",
    "\n",
    "# make context-target pairs for n-gram model\n",
    "def make_ngrams(corpus, n):\n",
    "    X, y = [], []\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words) - n):\n",
    "            context = words[i:i+n]\n",
    "            target = words[i+n]\n",
    "            X.append([word2idx[w] for w in context])\n",
    "            y.append(word2idx[target])\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "X, y = make_ngrams(corpus, n-1)\n",
    "\n",
    "\n",
    "print(\"Vocabulary size:\", V)\n",
    "print(\"Number of training samples:\", len(X))\n",
    "print('Example context:', X[0].tolist(), '-> target:', y[0].item())\n",
    "print('Example context words:', [idx2word[i] for i in X[0].tolist()])\n",
    "print('Example target word:', idx2word[y[0].item()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd51b29",
   "metadata": {},
   "source": [
    "#### Model architecture and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0f37161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Dataset/Dataloader\n",
    "class NGramDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(NGramDataset(X, y), batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e40a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tree(tree, prefix=\"\", paths=None, depths=None, freqs=None):\n",
    "    \"\"\"\n",
    "    Analyze tree paths, depths and frequencies recursively\n",
    "    If freqs is None, uses weight=1 for each word\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries\n",
    "    paths = {} if paths is None else paths\n",
    "    depths = {} if depths is None else depths\n",
    "    freqs = {} if freqs is None else freqs\n",
    "    \n",
    "    # If leaf node, record path, depth and frequency\n",
    "    if not tree.get('left') and not tree.get('right'):\n",
    "        word = tree['name']\n",
    "        freq = tree.get('freq', 1)  # Default frequency = 1 if not specified\n",
    "        paths[word] = prefix\n",
    "        depths[word] = len(prefix)\n",
    "        freqs[word] = freq\n",
    "        return paths, depths, freqs\n",
    "    \n",
    "    # Recurse on children\n",
    "    if tree.get('left'):\n",
    "        analyze_tree(tree['left'], prefix + \"0\", paths, depths, freqs)\n",
    "    if tree.get('right'):\n",
    "        analyze_tree(tree['right'], prefix + \"1\", paths, depths, freqs)\n",
    "    \n",
    "    return paths, depths, freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a41dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_strict_balanced(words, d):\n",
    "    counter = [0]  # Mutable counter for naming internal nodes\n",
    "    \n",
    "    def build_recursive(words):\n",
    "        if not words:\n",
    "            return None\n",
    "        if len(words) == 1:\n",
    "            return {\"name\": words[0]}  # Leaf node - no parameters needed\n",
    "            \n",
    "        # Internal node with parameters\n",
    "        node_name = f\"n{counter[0]}\"\n",
    "        counter[0] += 1\n",
    "        \n",
    "        mid = len(words)//2\n",
    "        node = {\n",
    "            \"name\": node_name,\n",
    "            \"alpha\": torch.zeros(1),  # Bias parameter\n",
    "            \"N\": torch.randn(1, d),   # Node embedding [1 x d_node]\n",
    "            \"left\": build_recursive(words[:mid]),\n",
    "            \"right\": build_recursive(words[mid:])\n",
    "        }\n",
    "        return node\n",
    "    \n",
    "    return build_recursive(words)\n",
    "\n",
    "# Create balanced tree with parameters\n",
    "balanced_tree = build_strict_balanced(words, d_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6e44b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Hierarchical Softmax Model\n",
    "# =============================\n",
    "class HierarchicalSoftmaxLM(nn.Module):\n",
    "    def __init__(self, vocab_size, n, m, h, d_node, word2idx, paths, tree):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.h = h\n",
    "        self.d_node = d_node\n",
    "        self.word2idx = word2idx\n",
    "        self.paths = paths\n",
    "        self.tree = tree\n",
    "        \n",
    "        # word embeddings\n",
    "        self.embeddings = nn.Embedding(vocab_size, m)\n",
    "\n",
    "        # shared parameters\n",
    "        self.W = nn.Linear((n-1)*m, h, bias=False)   # context → hidden\n",
    "        self.U = nn.Linear(d_node, h, bias=False)    # node embedding → hidden\n",
    "        self.c = nn.Parameter(torch.zeros(h))        # hidden bias\n",
    "        self.beta = nn.Parameter(torch.randn(h))     # projection vector β\n",
    "\n",
    "        # Register tree parameters\n",
    "        def register_tree_params(node):\n",
    "            if node is None or not (\"left\" in node or \"right\" in node):\n",
    "                return\n",
    "            # Convert node parameters to nn.Parameters\n",
    "            node[\"alpha\"] = nn.Parameter(node[\"alpha\"])\n",
    "            node[\"N\"] = nn.Parameter(node[\"N\"])\n",
    "            # Register parameters with PyTorch\n",
    "            self.register_parameter(f\"alpha_{node['name']}\", node[\"alpha\"])\n",
    "            self.register_parameter(f\"N_{node['name']}\", node[\"N\"])\n",
    "            register_tree_params(node.get(\"left\"))\n",
    "            register_tree_params(node.get(\"right\"))\n",
    "        \n",
    "        register_tree_params(self.tree)\n",
    "\n",
    "        return\n",
    "    \n",
    "\n",
    "    def forward(self, context_idxs, target_idxs):\n",
    "        B = context_idxs.size(0)\n",
    "        \n",
    "        # Context representation x\n",
    "        ctx_emb = self.embeddings(context_idxs)   # [B, n-1, m]\n",
    "        x = ctx_emb.view(B, -1)                   # [B, (n-1)*m]\n",
    "        \n",
    "        losses = []\n",
    "        for i in range(B):\n",
    "            target_word = idx2word[target_idxs[i].item()]\n",
    "            path = self.paths[target_word]\n",
    "            \n",
    "            loss_i = 0.0\n",
    "            curr_node = self.tree\n",
    "            for bit in path:\n",
    "                # Use parameters directly from tree node\n",
    "                alpha = curr_node[\"alpha\"]\n",
    "                N = curr_node[\"N\"]\n",
    "                \n",
    "                # Compute hidden state\n",
    "                h_in = self.W(x[i].unsqueeze(0)) + self.U(N) + self.c\n",
    "                h_out = torch.tanh(h_in)\n",
    "                \n",
    "                # Compute probability\n",
    "                p = torch.sigmoid(alpha + (h_out @ self.beta))\n",
    "                \n",
    "                # Binary cross entropy\n",
    "                b_val = float(bit == \"1\")\n",
    "                loss_i += -(b_val * torch.log(p + 1e-9) + (1-b_val) * torch.log(1-p + 1e-9))\n",
    "                \n",
    "                # Move to next node\n",
    "                curr_node = curr_node[\"right\"] if bit == \"1\" else curr_node[\"left\"]\n",
    "            \n",
    "            losses.append(loss_i)\n",
    "        \n",
    "        return torch.mean(torch.stack(losses))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d3d471a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "==================\n",
      "Vocabulary size (V): 42\n",
      "Context size (n-1): 3\n",
      "Embedding dim (m): 10\n",
      "Hidden dim (h): 16\n",
      "\n",
      "Model Parameter Counts:\n",
      "----------------------------------------\n",
      "Word Embeddings       : 420\n",
      "Context Matrix W      : 480\n",
      "Node Matrix U         : 256\n",
      "Hidden Bias c         : 16\n",
      "Projection Vector β   : 16\n",
      "Tree Node Parameters  : 697\n",
      "----------------------------------------\n",
      "Total Parameters (manual) : 1,885\n",
      "Total Parameters (model)  : 1,885\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Model definition\n",
    "# =============================\n",
    "\n",
    "device = torch.device(\"cpu\")  # force CPU for small data\n",
    "paths, depths, freqs = analyze_tree(balanced_tree)\n",
    "model = HierarchicalSoftmaxLM(V, n, m, h, d_node, word2idx, paths, balanced_tree).to(device)\n",
    "\n",
    "def count_model_parameters(model):\n",
    "    \"\"\"\n",
    "    Count and display parameter sizes for each component of the model\n",
    "    \"\"\"\n",
    "    def count_tree_params(node):\n",
    "        \"\"\"Count parameters in tree nodes recursively\"\"\"\n",
    "        if node is None or not (\"left\" in node or \"right\" in node):\n",
    "            return 0\n",
    "        # Count parameters in current node (alpha: 1, N: d_node)\n",
    "        node_params = 1 + node[\"N\"].numel()\n",
    "        # Add parameters from children\n",
    "        return node_params + count_tree_params(node.get(\"left\")) + count_tree_params(node.get(\"right\"))\n",
    "\n",
    "    # Count embedding parameters\n",
    "    emb_params = model.embeddings.weight.numel()\n",
    "    \n",
    "    # Count shared parameters\n",
    "    w_params = model.W.weight.numel()\n",
    "    u_params = model.U.weight.numel()\n",
    "    c_params = model.c.numel()\n",
    "    beta_params = model.beta.numel()\n",
    "    \n",
    "    # Count tree parameters\n",
    "    tree_params = count_tree_params(model.tree)\n",
    "    \n",
    "    # Print parameter counts\n",
    "    print(\"\\nModel Parameter Counts:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Word Embeddings       : {emb_params:,d}\")\n",
    "    print(f\"Context Matrix W      : {w_params:,d}\")\n",
    "    print(f\"Node Matrix U         : {u_params:,d}\")\n",
    "    print(f\"Hidden Bias c         : {c_params:,d}\")\n",
    "    print(f\"Projection Vector β   : {beta_params:,d}\")\n",
    "    print(f\"Tree Node Parameters  : {tree_params:,d}\")\n",
    "    print(\"-\" * 40)\n",
    "    total_params_manual = emb_params + w_params + u_params + c_params + beta_params + tree_params\n",
    "    total_params_model = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    print(f\"Total Parameters (manual) : {total_params_manual:,d}\")\n",
    "    print(f\"Total Parameters (model)  : {total_params_model:,d}\")\n",
    "\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(\"==================\")\n",
    "print(f\"Vocabulary size (V): {V}\")\n",
    "print(f\"Context size (n-1): {n-1}\")\n",
    "print(f\"Embedding dim (m): {m}\")\n",
    "print(f\"Hidden dim (h): {h}\")\n",
    "\n",
    "# Use after model creation\n",
    "count_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0646b0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.6902\n",
      "Epoch 2: Loss = 0.6781\n",
      "Epoch 3: Loss = 0.4924\n",
      "Epoch 4: Loss = 0.3964\n",
      "Epoch 5: Loss = 0.3547\n",
      "Epoch 6: Loss = 0.2692\n",
      "Epoch 7: Loss = 0.2434\n",
      "Epoch 8: Loss = 0.2098\n",
      "Epoch 9: Loss = 0.1765\n",
      "Epoch 10: Loss = 0.1515\n",
      "Epoch 11: Loss = 0.1419\n",
      "Epoch 12: Loss = 0.1228\n",
      "Epoch 13: Loss = 0.1106\n",
      "Epoch 14: Loss = 0.1022\n",
      "Epoch 15: Loss = 0.0954\n",
      "Epoch 16: Loss = 0.0868\n",
      "Epoch 17: Loss = 0.0792\n",
      "Epoch 18: Loss = 0.0741\n",
      "Epoch 19: Loss = 0.0744\n",
      "Epoch 20: Loss = 0.0682\n",
      "Epoch 21: Loss = 0.0644\n",
      "Epoch 22: Loss = 0.0583\n",
      "Epoch 23: Loss = 0.0564\n",
      "Epoch 24: Loss = 0.0511\n",
      "Epoch 25: Loss = 0.0516\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Training\n",
    "# =============================\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")  # force CPU for small data\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in train_loader:\n",
    "        context, target = context.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(context, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f2b4d",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd4da33",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 67\u001b[0m\n\u001b[1;32m     59\u001b[0m test_contexts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     60\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msat\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     61\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchased\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     62\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdog\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbarked\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     63\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstars\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglows\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     64\u001b[0m ]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m context \u001b[38;5;129;01min\u001b[39;00m test_contexts:\n\u001b[0;32m---> 67\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mContext: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (word, prob) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(predictions, \u001b[38;5;241m1\u001b[39m):\n",
      "Cell \u001b[0;32mIn[27], line 50\u001b[0m, in \u001b[0;36mpredict_next\u001b[0;34m(context_words, k)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prob\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Compute probabilities for all words\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m word_probs \u001b[38;5;241m=\u001b[39m [(word, get_word_prob(word, x)) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Sort by probability and get top k\u001b[39;00m\n\u001b[1;32m     53\u001b[0m word_probs\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[27], line 50\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prob\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Compute probabilities for all words\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m word_probs \u001b[38;5;241m=\u001b[39m [(word, \u001b[43mget_word_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Sort by probability and get top k\u001b[39;00m\n\u001b[1;32m     53\u001b[0m word_probs\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[27], line 47\u001b[0m, in \u001b[0;36mpredict_next.<locals>.get_word_prob\u001b[0;34m(word, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m     curr_node \u001b[38;5;241m=\u001b[39m curr_node[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m bit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m curr_node[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     45\u001b[0m prob1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(log_prob)\u001b[38;5;241m.\u001b[39mitem()                 \n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprob\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[27], line 47\u001b[0m, in \u001b[0;36mpredict_next.<locals>.get_word_prob\u001b[0;34m(word, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m     curr_node \u001b[38;5;241m=\u001b[39m curr_node[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m bit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m curr_node[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     45\u001b[0m prob1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(log_prob)\u001b[38;5;241m.\u001b[39mitem()                 \n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprob\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/graphrag/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/graphrag/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def predict_next(context_words, k=5):\n",
    "    \"\"\"\n",
    "    Predict next word probabilities using hierarchical softmax tree\n",
    "    Args:\n",
    "        context_words: List of context words\n",
    "        k: Number of top predictions to return\n",
    "    Returns:\n",
    "        List of (word, probability) tuples\n",
    "    \"\"\"\n",
    "    context_idxs = torch.tensor([[word2idx[w] for w in context_words]])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get context embedding\n",
    "        ctx_emb = model.embeddings(context_idxs)   # [1, n-1, m]\n",
    "        x = ctx_emb.view(1, -1)                    # [1, (n-1)*m]\n",
    "        \n",
    "        def get_node_prob(node, x):\n",
    "            \"\"\"Compute probability of taking right path at node\"\"\"\n",
    "            if not node or not (\"left\" in node or \"right\" in node):\n",
    "                return 1.0\n",
    "            \n",
    "            # Get node parameters\n",
    "            alpha = node[\"alpha\"]\n",
    "            N = node[\"N\"]\n",
    "            \n",
    "            # Compute hidden state\n",
    "            h_in = model.W(x) + model.U(N) + model.c\n",
    "            h_out = torch.tanh(h_in)\n",
    "            \n",
    "            # Return probability of going right\n",
    "            return torch.sigmoid(alpha + (h_out @ model.beta))\n",
    "        \n",
    "        def get_word_prob(word, x):\n",
    "            \"\"\"Compute probability of word by multiplying path probabilities\"\"\"\n",
    "            path = model.paths[word]\n",
    "            curr_node = model.tree\n",
    "            log_prob = 0.0\n",
    "            \n",
    "            for bit in path:\n",
    "                p = get_node_prob(curr_node, x)\n",
    "                log_prob += torch.log(p if bit == \"1\" else (1 - p))\n",
    "                curr_node = curr_node[\"right\"] if bit == \"1\" else curr_node[\"left\"]\n",
    "            prob = torch.exp(log_prob).item()                 \n",
    "            \n",
    "            return prob\n",
    "        \n",
    "        # Compute probabilities for all words\n",
    "        word_probs = [(word, get_word_prob(word, x)) for word in words]\n",
    "        \n",
    "        # Sort by probability and get top k\n",
    "        word_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "        predictions = word_probs[:k]\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Test predictions\n",
    "test_contexts = [\n",
    "    [\"the\", \"cat\", \"sat\"],\n",
    "    [\"a\", \"cat\", \"chased\"],\n",
    "    [\"a\", \"dog\", \"barked\"],\n",
    "    [\"the\", \"stars\", \"glows\"]\n",
    "]\n",
    "\n",
    "for context in test_contexts:\n",
    "    predictions = predict_next(context)\n",
    "    print(f\"\\nContext: {context}\")\n",
    "    for i, (word, prob) in enumerate(predictions, 1):\n",
    "        print(f\"  {i}. '{word}' ({prob:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
