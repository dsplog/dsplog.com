{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f872da38",
   "metadata": {},
   "source": [
    "## Neural Probabilitistic Language Model with Noise Contrastive Estimation (NCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c197405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Neural Probabilistic Language Model \n",
    "# Bengio et al. (2003) \n",
    "# =========================\n",
    "\n",
    "# 1. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 2. Config\n",
    "n = 4        # context size = n-1 previous words + 1 target\n",
    "m = 10       # embedding dimension [m x 1]\n",
    "h = 16       # hidden layer dimension [h x 1]\n",
    "epochs = 20\n",
    "lr = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c8e88",
   "metadata": {},
   "source": [
    "#### Toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d03db988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 42\n",
      "Number of training samples: 22\n",
      "Example context: [39, 9, 30] -> target: 12\n",
      "Example context words: ['the', 'cat', 'sat']\n",
      "Example target word: down\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 3. Toy Corpus (~20 sentences)\n",
    "# -----------------------------\n",
    "corpus = [\n",
    "    \"the cat sat down\",\n",
    "    \"the cat ate food\",\n",
    "    \"the dog sat down\",\n",
    "    \"the dog ate food\",\n",
    "    \"a cat chased a mouse\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"a dog barked loudly\",\n",
    "    \"the cat meowed softly\",\n",
    "    \"the bird sang sweetly\",\n",
    "    \"a bird flew away\",\n",
    "    \"the fish swam fast\",\n",
    "    \"a fish jumped high\",\n",
    "    \"the boy played ball\",\n",
    "    \"the girl sang song\",\n",
    "    \"a boy read book\",\n",
    "    \"a girl wrote letter\",\n",
    "    \"the sun shines bright\",\n",
    "    \"the moon glows softly\",\n",
    "    \"the stars twinkle bright\",\n",
    "    \"a cat slept quietly\"\n",
    "]\n",
    "\n",
    "\n",
    "# 4. Preprocessing\n",
    "tokens = set(\" \".join(corpus).split())\n",
    "word2idx = {word: i for i, word in enumerate(sorted(tokens))}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "V = len(word2idx)   # vocabulary size |V|\n",
    "\n",
    "# make context-target pairs for n-gram model\n",
    "def make_ngrams(corpus, n):\n",
    "    X, y = [], []\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words) - n):\n",
    "            context = words[i:i+n]\n",
    "            target = words[i+n]\n",
    "            X.append([word2idx[w] for w in context])\n",
    "            y.append(word2idx[target])\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "X, y = make_ngrams(corpus, n-1)\n",
    "\n",
    "\n",
    "print(\"Vocabulary size:\", V)\n",
    "print(\"Number of training samples:\", len(X))\n",
    "print('Example context:', X[0].tolist(), '-> target:', y[0].item())\n",
    "print('Example context words:', [idx2word[i] for i in X[0].tolist()])\n",
    "print('Example target word:', idx2word[y[0].item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f276d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "counts = collections.Counter(\" \".join(corpus).split())\n",
    "total = sum(counts.values())\n",
    "noise_dist = torch.tensor([counts[w]/total for w in tokens], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd51b29",
   "metadata": {},
   "source": [
    "#### Model architecture and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a0f37161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Dataset/Dataloader\n",
    "class NGramDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(NGramDataset(X, y), batch_size=4, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c46c4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NPLM_NCE_ContextZ(nn.Module):\n",
    "    def __init__(self, V, m, n, h, noise_dist, k):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.V = V\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.h = h\n",
    "        self.k = k\n",
    "        self.noise_dist = noise_dist  # torch tensor of size [V]\n",
    "        \n",
    "        # --- Core NPLM parameters ---\n",
    "        self.C = nn.Embedding(V, m)\n",
    "        self.W = nn.Linear((n-1)*m, V, bias=False)\n",
    "        self.H = nn.Linear((n-1)*m, h, bias=False)\n",
    "        self.U = nn.Linear(h, V, bias=False)\n",
    "        self.b = nn.Parameter(torch.zeros(V))\n",
    "        self.d = nn.Parameter(torch.zeros(h))\n",
    "        \n",
    "        # --- Context-dependent log Z(x) layer ---\n",
    "        # This predicts logZ from flattened context embedding\n",
    "        self.Z_layer = nn.Linear((n-1)*m, 1, bias=True)\n",
    "        \n",
    "    def score(self, x):\n",
    "        \"\"\"Compute unnormalized log fÎ¸(x, w)\"\"\"\n",
    "        embeddings = self.C(x)                     # [batch, n-1, m]\n",
    "        x_flat = embeddings.view(embeddings.size(0), -1)  # [batch, (n-1)*m]\n",
    "        h_tanh = torch.tanh(self.d + self.H(x_flat))      # [batch, h]\n",
    "        y = self.b + self.W(x_flat) + self.U(h_tanh)      # [batch, V]\n",
    "        return y, x_flat\n",
    "    \n",
    "    def forward(self, context, target):\n",
    "        \"\"\"\n",
    "        context: [batch, n-1]\n",
    "        target:  [batch]\n",
    "        \"\"\"\n",
    "        batch_size = context.size(0)\n",
    "        logits, x_flat = self.score(context)  # [batch, V], [batch, (n-1)*m]\n",
    "        \n",
    "        # Predict log Z(x) for each context\n",
    "        logZ = self.Z_layer(x_flat).squeeze(1)  # [batch]\n",
    "        \n",
    "        # Positive samples\n",
    "        pos_logits = logits.gather(1, target.view(-1, 1)).squeeze(1)  # [batch]\n",
    "        \n",
    "        # Negative samples\n",
    "        noise_samples = torch.multinomial(\n",
    "            self.noise_dist, batch_size * self.k, replacement=True\n",
    "        ).view(batch_size, self.k)\n",
    "        noise_logits = logits.gather(1, noise_samples)  # [batch, k]\n",
    "        \n",
    "        # log(k * Pn(w))\n",
    "        log_kPn_pos = torch.log(self.k * self.noise_dist[target])\n",
    "        log_kPn_neg = torch.log(self.k * self.noise_dist[noise_samples])\n",
    "        \n",
    "        # Adjusted scores with context-dependent normalization\n",
    "        pos_score = pos_logits - logZ - log_kPn_pos\n",
    "        neg_score = noise_logits - logZ.unsqueeze(1) - log_kPn_neg\n",
    "        \n",
    "        # NCE loss\n",
    "        pos_loss = F.logsigmoid(pos_score)\n",
    "        neg_loss = torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
    "        \n",
    "        loss = - (pos_loss + neg_loss).mean()\n",
    "        return loss, logZ.detach()\n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c61a56",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c5995649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 9.8931\n",
      "Epoch 2 | Loss: 7.6272\n",
      "Epoch 3 | Loss: 2.9952\n",
      "Epoch 4 | Loss: 4.7679\n",
      "Epoch 5 | Loss: 2.5692\n",
      "Epoch 6 | Loss: 3.0233\n",
      "Epoch 7 | Loss: 1.3058\n",
      "Epoch 8 | Loss: 1.0921\n",
      "Epoch 9 | Loss: 1.4352\n",
      "Epoch 10 | Loss: 0.9338\n",
      "Epoch 11 | Loss: 1.0145\n",
      "Epoch 12 | Loss: 0.9510\n",
      "Epoch 13 | Loss: 1.3104\n",
      "Epoch 14 | Loss: 1.1549\n",
      "Epoch 15 | Loss: 1.5295\n",
      "Epoch 16 | Loss: 0.3475\n",
      "Epoch 17 | Loss: 0.4630\n",
      "Epoch 18 | Loss: 0.8477\n",
      "Epoch 19 | Loss: 0.2316\n",
      "Epoch 20 | Loss: 1.3759\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming corpus already tokenized and dataloader prepared\n",
    "model = NPLM_NCE_ContextZ(V, m, n, h, noise_dist, k=5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "logZ_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for context, target in train_loader:\n",
    "        loss, logZ = model(context, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        logZ_history.extend(logZ.tolist())\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26094c71",
   "metadata": {},
   "source": [
    "#### Probability of next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "badeeb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: ['the', 'cat', 'sat']\n",
      "  1. 'down' (2.8710)\n",
      "  2. 'softly' (0.2379)\n",
      "  3. 'quietly' (0.0942)\n",
      "  4. 'bright' (0.0358)\n",
      "  5. 'ball' (0.0256)\n",
      "\n",
      "Context: ['a', 'cat', 'chased']\n",
      "  1. 'a' (0.3181)\n",
      "  2. 'quietly' (0.1563)\n",
      "  3. 'letter' (0.0671)\n",
      "  4. 'away' (0.0117)\n",
      "  5. 'the' (0.0115)\n",
      "\n",
      "Context: ['a', 'dog', 'barked']\n",
      "  1. 'loudly' (0.1600)\n",
      "  2. 'letter' (0.0455)\n",
      "  3. 'a' (0.0150)\n",
      "  4. 'book' (0.0118)\n",
      "  5. 'the' (0.0110)\n",
      "\n",
      "Context: ['the', 'stars', 'glows']\n",
      "  1. 'bright' (0.1753)\n",
      "  2. 'fast' (0.0784)\n",
      "  3. 'softly' (0.0365)\n",
      "  4. 'food' (0.0095)\n",
      "  5. 'away' (0.0081)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6. Prediction Function\n",
    "# -----------------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_next(self, context, top_k=5):\n",
    "    \"\"\"\n",
    "    Predict next word probabilities given context.\n",
    "    context: [1, n-1] tensor of word indices\n",
    "    \"\"\"\n",
    "    logits, x_flat = model.score(context)  # [1, V], [1, (n-1)*m]\n",
    "    \n",
    "    # Get learned logZ(x)\n",
    "    logZ = model.Z_layer(x_flat).squeeze(1)  # scalar per batch\n",
    "    \n",
    "    # Get unnormalized log probabilities by subtracting learned logZ\n",
    "    log_probs = logits - logZ.unsqueeze(1)\n",
    "    # Convert to probabilities without re-normalizing (exp is monotonic, so ranking is preserved)\n",
    "    probs = torch.exp(log_probs)\n",
    "    \n",
    "    # Top-k predictions\n",
    "    top_probs, top_indices = probs.topk(top_k, dim=1)\n",
    "    \n",
    "    # Convert indices to words and return with probabilities\n",
    "    predictions = [(idx2word[idx.item()], prob.item()) for idx, prob in zip(top_indices.squeeze(0), top_probs.squeeze(0))]\n",
    "    return predictions\n",
    "\n",
    "# Test a few predictions\n",
    "test_contexts = [\n",
    "    [\"the\", \"cat\", \"sat\"],\n",
    "    [\"a\", \"cat\", \"chased\"],\n",
    "    [\"a\",\"dog\",\"barked\"],\n",
    "    [\"the\",\"stars\",\"glows\"]\n",
    "]\n",
    "\n",
    "for context in test_contexts:\n",
    "    # Convert words to indices\n",
    "    context_indices = torch.tensor([[word2idx[w] for w in context]])\n",
    "    predictions = predict_next(model, context_indices)\n",
    "    print(f\"\\nContext: {context}\")\n",
    "    for i, (word, prob) in enumerate(predictions, 1):\n",
    "        print(f\"  {i}. '{word}' ({prob:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
